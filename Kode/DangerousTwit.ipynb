{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d0015",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b99b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobertweet-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"indolem/indobertweet-base-uncased\")\n",
    "\n",
    "def extract_features(text):\n",
    "    if isinstance(text, str):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return None\n",
    "\n",
    "df['features'] = df['full_text'].apply(extract_features)\n",
    "print(df['features'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "259948e2-4b69-4f5e-9535-d08de93b5526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/data_fix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba60d04-1db0-4031-8965-c0f35d04ca8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>id_str</th>\n",
       "      <th>image_url</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>indobert_feature_71</th>\n",
       "      <th>indobert_feature_72</th>\n",
       "      <th>indobert_feature_73</th>\n",
       "      <th>indobert_feature_74</th>\n",
       "      <th>indobert_feature_75</th>\n",
       "      <th>indobert_feature_76</th>\n",
       "      <th>indobert_feature_77</th>\n",
       "      <th>indobert_feature_78</th>\n",
       "      <th>indobert_feature_79</th>\n",
       "      <th>indobert_feature_80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.538779e+18</td>\n",
       "      <td>2022-06-20 07:01:14+00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>#JokowiMatikanDemokrasi #JokowiMatikanDemokrasi</td>\n",
       "      <td>1.538779e+18</td>\n",
       "      <td>https://pbs.twimg.com/media/FVrXVWnUYAEBDfI.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312094</td>\n",
       "      <td>0.980298</td>\n",
       "      <td>-0.497925</td>\n",
       "      <td>1.602446</td>\n",
       "      <td>-0.254567</td>\n",
       "      <td>-0.520526</td>\n",
       "      <td>-0.353177</td>\n",
       "      <td>-1.112838</td>\n",
       "      <td>-0.001756</td>\n",
       "      <td>-0.027748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.538783e+18</td>\n",
       "      <td>2022-06-20 07:16:30+00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Polisi kalah sama lont... #JokowiMatikanDemokr...</td>\n",
       "      <td>1.538783e+18</td>\n",
       "      <td>https://pbs.twimg.com/media/FVra02UUAAAJ4-R.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.409329</td>\n",
       "      <td>0.674773</td>\n",
       "      <td>-0.127567</td>\n",
       "      <td>0.804900</td>\n",
       "      <td>-0.329457</td>\n",
       "      <td>-0.290906</td>\n",
       "      <td>-0.106401</td>\n",
       "      <td>0.068177</td>\n",
       "      <td>0.417723</td>\n",
       "      <td>-0.022249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.538783e+18</td>\n",
       "      <td>2022-06-20 07:17:55+00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Setuju #JokowiMatikanDemokrasi #JokowiMatikanD...</td>\n",
       "      <td>1.538783e+18</td>\n",
       "      <td>https://pbs.twimg.com/media/FVrbJuBUYAAbmJH.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204514</td>\n",
       "      <td>0.745370</td>\n",
       "      <td>-0.635906</td>\n",
       "      <td>1.242655</td>\n",
       "      <td>-0.276266</td>\n",
       "      <td>-0.039602</td>\n",
       "      <td>-0.501906</td>\n",
       "      <td>-1.205206</td>\n",
       "      <td>0.055984</td>\n",
       "      <td>-0.102962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.538783e+18</td>\n",
       "      <td>2022-06-20 07:18:41+00:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Bubarkan KPK. #JokowiMatikanDemokrasi #JokowiM...</td>\n",
       "      <td>1.538783e+18</td>\n",
       "      <td>https://pbs.twimg.com/media/FVrbU4oUUAE3aS8.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228128</td>\n",
       "      <td>0.552171</td>\n",
       "      <td>-0.218623</td>\n",
       "      <td>1.189514</td>\n",
       "      <td>-0.291000</td>\n",
       "      <td>-0.466457</td>\n",
       "      <td>-0.305309</td>\n",
       "      <td>-0.516276</td>\n",
       "      <td>0.302189</td>\n",
       "      <td>0.132921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.538786e+18</td>\n",
       "      <td>2022-06-20 07:27:45+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rakyat maunya jokowi legowo MUNDUR. #JokowiMat...</td>\n",
       "      <td>1.538786e+18</td>\n",
       "      <td>https://pbs.twimg.com/media/FVrdZ7iUUAQBg2M.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081445</td>\n",
       "      <td>0.669836</td>\n",
       "      <td>-0.144411</td>\n",
       "      <td>0.584860</td>\n",
       "      <td>-0.471251</td>\n",
       "      <td>-0.212964</td>\n",
       "      <td>-0.327029</td>\n",
       "      <td>-0.617646</td>\n",
       "      <td>0.011081</td>\n",
       "      <td>0.291943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   conversation_id_str                 created_at  favorite_count  \\\n",
       "0         1.538779e+18  2022-06-20 07:01:14+00:00             5.0   \n",
       "1         1.538783e+18  2022-06-20 07:16:30+00:00             4.0   \n",
       "2         1.538783e+18  2022-06-20 07:17:55+00:00            10.0   \n",
       "3         1.538783e+18  2022-06-20 07:18:41+00:00            20.0   \n",
       "4         1.538786e+18  2022-06-20 07:27:45+00:00             0.0   \n",
       "\n",
       "                                           full_text        id_str  \\\n",
       "0    #JokowiMatikanDemokrasi #JokowiMatikanDemokrasi  1.538779e+18   \n",
       "1  Polisi kalah sama lont... #JokowiMatikanDemokr...  1.538783e+18   \n",
       "2  Setuju #JokowiMatikanDemokrasi #JokowiMatikanD...  1.538783e+18   \n",
       "3  Bubarkan KPK. #JokowiMatikanDemokrasi #JokowiM...  1.538783e+18   \n",
       "4  Rakyat maunya jokowi legowo MUNDUR. #JokowiMat...  1.538786e+18   \n",
       "\n",
       "                                         image_url in_reply_to_screen_name  \\\n",
       "0  https://pbs.twimg.com/media/FVrXVWnUYAEBDfI.jpg                     NaN   \n",
       "1  https://pbs.twimg.com/media/FVra02UUAAAJ4-R.jpg                     NaN   \n",
       "2  https://pbs.twimg.com/media/FVrbJuBUYAAbmJH.jpg                     NaN   \n",
       "3  https://pbs.twimg.com/media/FVrbU4oUUAE3aS8.jpg                     NaN   \n",
       "4  https://pbs.twimg.com/media/FVrdZ7iUUAQBg2M.jpg                     NaN   \n",
       "\n",
       "  lang location  quote_count  ...  indobert_feature_71  indobert_feature_72  \\\n",
       "0  qme      NaN          0.0  ...            -0.312094             0.980298   \n",
       "1   in      NaN          0.0  ...            -0.409329             0.674773   \n",
       "2   in      NaN          0.0  ...            -0.204514             0.745370   \n",
       "3   in      NaN          3.0  ...            -0.228128             0.552171   \n",
       "4   in      NaN          0.0  ...            -0.081445             0.669836   \n",
       "\n",
       "  indobert_feature_73  indobert_feature_74 indobert_feature_75  \\\n",
       "0           -0.497925             1.602446           -0.254567   \n",
       "1           -0.127567             0.804900           -0.329457   \n",
       "2           -0.635906             1.242655           -0.276266   \n",
       "3           -0.218623             1.189514           -0.291000   \n",
       "4           -0.144411             0.584860           -0.471251   \n",
       "\n",
       "  indobert_feature_76 indobert_feature_77 indobert_feature_78  \\\n",
       "0           -0.520526           -0.353177           -1.112838   \n",
       "1           -0.290906           -0.106401            0.068177   \n",
       "2           -0.039602           -0.501906           -1.205206   \n",
       "3           -0.466457           -0.305309           -0.516276   \n",
       "4           -0.212964           -0.327029           -0.617646   \n",
       "\n",
       "   indobert_feature_79  indobert_feature_80  \n",
       "0            -0.001756            -0.027748  \n",
       "1             0.417723            -0.022249  \n",
       "2             0.055984            -0.102962  \n",
       "3             0.302189             0.132921  \n",
       "4             0.011081             0.291943  \n",
       "\n",
       "[5 rows x 203 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc2cb4d-dbbe-4e57-bef4-b5f80ab7cd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversation_id_str\n",
      "created_at\n",
      "favorite_count\n",
      "full_text\n",
      "id_str\n",
      "image_url\n",
      "in_reply_to_screen_name\n",
      "lang\n",
      "location\n",
      "quote_count\n",
      "reply_count\n",
      "retweet_count\n",
      "tweet_url\n",
      "user_id_str\n",
      "username\n",
      "full_text_clean\n",
      "full_text_clean_2\n",
      "features\n",
      "cluster\n",
      "ttr\n",
      "avg_word_length\n",
      "avg_syllables_per_word\n",
      "avg_word_frequency\n",
      "functional_word_count\n",
      "uber_index\n",
      "hd_d_index\n",
      "tuldava_measure_u\n",
      "yules_k\n",
      "shannon_entropy\n",
      "simpsons_index\n",
      "flesch_kincaid\n",
      "dale_chall\n",
      "linsear_write\n",
      "gunning_fog\n",
      "double_dot_count\n",
      "url_count\n",
      "hashtag_count\n",
      "mention_count\n",
      "pca_feature_1\n",
      "pca_feature_2\n",
      "pca_feature_3\n",
      "pca_feature_4\n",
      "pca_feature_5\n",
      "pca_feature_6\n",
      "pca_feature_7\n",
      "pca_feature_8\n",
      "pca_feature_9\n",
      "pca_feature_10\n",
      "pca_feature_11\n",
      "pca_feature_12\n",
      "pca_feature_13\n",
      "pca_feature_14\n",
      "pca_feature_15\n",
      "pca_feature_16\n",
      "pca_feature_17\n",
      "pca_feature_18\n",
      "pca_feature_19\n",
      "kmeans_cluster\n",
      "gmm_cluster\n",
      "word_count\n",
      "comma_count\n",
      "comma_ratio\n",
      "period_count\n",
      "period_ratio\n",
      "semicolon_count\n",
      "semicolon_ratio\n",
      "colon_count\n",
      "colon_ratio\n",
      "question_mark_count\n",
      "question_mark_ratio\n",
      "exclamation_mark_count\n",
      "exclamation_mark_ratio\n",
      "dash_count\n",
      "dash_ratio\n",
      "parentheses_open_count\n",
      "parentheses_open_ratio\n",
      "parentheses_close_count\n",
      "parentheses_close_ratio\n",
      "brackets_open_count\n",
      "brackets_open_ratio\n",
      "brackets_close_count\n",
      "brackets_close_ratio\n",
      "quotation_count\n",
      "quotation_ratio\n",
      "ellipsis_count\n",
      "ellipsis_ratio\n",
      "total_punct_count\n",
      "total_punct_ratio\n",
      "sequential_punct_count\n",
      "sentence_initial_punct\n",
      "nonstandard_sentence_end\n",
      "punct_diversity\n",
      "function_word_count\n",
      "content_word_count\n",
      "function_ratio\n",
      "content_ratio\n",
      "function_to_content_ratio\n",
      "unique_function_words\n",
      "prepositions_count\n",
      "prepositions_ratio\n",
      "conjunctions_count\n",
      "conjunctions_ratio\n",
      "pronouns_count\n",
      "pronouns_ratio\n",
      "auxiliaries_count\n",
      "auxiliaries_ratio\n",
      "author\n",
      "marker_jadi\n",
      "marker_oleh_karena_itu\n",
      "marker_maka_dari_itu\n",
      "marker_menurut_saya\n",
      "marker_bagi_saya\n",
      "marker_faktanya\n",
      "total_markers_count\n",
      "markers_ratio\n",
      "top_first_word\n",
      "top_first_word_freq\n",
      "top_last_word\n",
      "top_last_word_freq\n",
      "favorite_2gram\n",
      "favorite_2gram_freq\n",
      "favorite_3gram\n",
      "favorite_3gram_freq\n",
      "indobert_feature_1\n",
      "indobert_feature_2\n",
      "indobert_feature_3\n",
      "indobert_feature_4\n",
      "indobert_feature_5\n",
      "indobert_feature_6\n",
      "indobert_feature_7\n",
      "indobert_feature_8\n",
      "indobert_feature_9\n",
      "indobert_feature_10\n",
      "indobert_feature_11\n",
      "indobert_feature_12\n",
      "indobert_feature_13\n",
      "indobert_feature_14\n",
      "indobert_feature_15\n",
      "indobert_feature_16\n",
      "indobert_feature_17\n",
      "indobert_feature_18\n",
      "indobert_feature_19\n",
      "indobert_feature_20\n",
      "indobert_feature_21\n",
      "indobert_feature_22\n",
      "indobert_feature_23\n",
      "indobert_feature_24\n",
      "indobert_feature_25\n",
      "indobert_feature_26\n",
      "indobert_feature_27\n",
      "indobert_feature_28\n",
      "indobert_feature_29\n",
      "indobert_feature_30\n",
      "indobert_feature_31\n",
      "indobert_feature_32\n",
      "indobert_feature_33\n",
      "indobert_feature_34\n",
      "indobert_feature_35\n",
      "indobert_feature_36\n",
      "indobert_feature_37\n",
      "indobert_feature_38\n",
      "indobert_feature_39\n",
      "indobert_feature_40\n",
      "indobert_feature_41\n",
      "indobert_feature_42\n",
      "indobert_feature_43\n",
      "indobert_feature_44\n",
      "indobert_feature_45\n",
      "indobert_feature_46\n",
      "indobert_feature_47\n",
      "indobert_feature_48\n",
      "indobert_feature_49\n",
      "indobert_feature_50\n",
      "indobert_feature_51\n",
      "indobert_feature_52\n",
      "indobert_feature_53\n",
      "indobert_feature_54\n",
      "indobert_feature_55\n",
      "indobert_feature_56\n",
      "indobert_feature_57\n",
      "indobert_feature_58\n",
      "indobert_feature_59\n",
      "indobert_feature_60\n",
      "indobert_feature_61\n",
      "indobert_feature_62\n",
      "indobert_feature_63\n",
      "indobert_feature_64\n",
      "indobert_feature_65\n",
      "indobert_feature_66\n",
      "indobert_feature_67\n",
      "indobert_feature_68\n",
      "indobert_feature_69\n",
      "indobert_feature_70\n",
      "indobert_feature_71\n",
      "indobert_feature_72\n",
      "indobert_feature_73\n",
      "indobert_feature_74\n",
      "indobert_feature_75\n",
      "indobert_feature_76\n",
      "indobert_feature_77\n",
      "indobert_feature_78\n",
      "indobert_feature_79\n",
      "indobert_feature_80\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0dd3bb-cb97-418b-9a02-595226e9f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indobert = [\n",
    "    'indobert_feature_1', 'indobert_feature_2', 'indobert_feature_3', \n",
    "    'indobert_feature_4', 'indobert_feature_5', 'indobert_feature_6', \n",
    "    'indobert_feature_7', 'indobert_feature_8', 'indobert_feature_9', \n",
    "    'indobert_feature_10', 'indobert_feature_11', 'indobert_feature_12', \n",
    "    'indobert_feature_13', 'indobert_feature_14', 'indobert_feature_15', \n",
    "    'indobert_feature_16', 'indobert_feature_17', 'indobert_feature_18', \n",
    "    'indobert_feature_19', 'indobert_feature_20', 'indobert_feature_21', \n",
    "    'indobert_feature_22', 'indobert_feature_23', 'indobert_feature_24', \n",
    "    'indobert_feature_25', 'indobert_feature_26', 'indobert_feature_27', \n",
    "    'indobert_feature_28', 'indobert_feature_29', 'indobert_feature_30', \n",
    "    'indobert_feature_31', 'indobert_feature_32', 'indobert_feature_33', \n",
    "    'indobert_feature_34', 'indobert_feature_35', 'indobert_feature_36', \n",
    "    'indobert_feature_37', 'indobert_feature_38', 'indobert_feature_39', \n",
    "    'indobert_feature_40', 'indobert_feature_41', 'indobert_feature_42', \n",
    "    'indobert_feature_43', 'indobert_feature_44', 'indobert_feature_45', \n",
    "    'indobert_feature_46', 'indobert_feature_47', 'indobert_feature_48', \n",
    "    'indobert_feature_49', 'indobert_feature_50', 'indobert_feature_51', \n",
    "    'indobert_feature_52', 'indobert_feature_53', 'indobert_feature_54', \n",
    "    'indobert_feature_55', 'indobert_feature_56', 'indobert_feature_57', \n",
    "    'indobert_feature_58', 'indobert_feature_59', 'indobert_feature_60', \n",
    "    'indobert_feature_61', 'indobert_feature_62', 'indobert_feature_63', \n",
    "    'indobert_feature_64',  'indobert_feature_65',\n",
    " 'indobert_feature_66',\n",
    " 'indobert_feature_67',\n",
    " 'indobert_feature_68',\n",
    " 'indobert_feature_69',\n",
    " 'indobert_feature_70',\n",
    " 'indobert_feature_71',\n",
    " 'indobert_feature_72',\n",
    " 'indobert_feature_73',\n",
    " 'indobert_feature_74',\n",
    " 'indobert_feature_75',\n",
    " 'indobert_feature_76',\n",
    " 'indobert_feature_77',\n",
    " 'indobert_feature_78',\n",
    " 'indobert_feature_79',\n",
    " 'indobert_feature_80'\n",
    "]\n",
    "\n",
    "stilistik = [\n",
    "    # Penggunaan tanda baca\n",
    "    'comma_count', 'comma_ratio',\n",
    "    'period_count', 'period_ratio',\n",
    "    'semicolon_count', 'semicolon_ratio',\n",
    "    'colon_count', 'colon_ratio',\n",
    "    'question_mark_count', 'question_mark_ratio',\n",
    "    'exclamation_mark_count', 'exclamation_mark_ratio',\n",
    "    'dash_count', 'dash_ratio',\n",
    "    'parentheses_open_count', 'parentheses_open_ratio',\n",
    "    'parentheses_close_count', 'parentheses_close_ratio',\n",
    "    'brackets_open_count', 'brackets_open_ratio',\n",
    "    'brackets_close_count', 'brackets_close_ratio',\n",
    "    'quotation_count', 'quotation_ratio',\n",
    "    'ellipsis_count', 'ellipsis_ratio',\n",
    "\n",
    "    'total_punct_count', 'total_punct_ratio',\n",
    "    'sequential_punct_count', 'sentence_initial_punct',\n",
    "    'nonstandard_sentence_end', 'punct_diversity',\n",
    "    \n",
    "    # Jenis kata\n",
    "    'function_word_count', 'content_word_count',\n",
    "    'function_ratio', 'content_ratio',\n",
    "    'function_to_content_ratio', 'unique_function_words',\n",
    "    'functional_word_count',\n",
    "    'prepositions_count', 'prepositions_ratio',\n",
    "    'conjunctions_count', 'conjunctions_ratio',\n",
    "    'pronouns_count', 'pronouns_ratio',\n",
    "    'auxiliaries_count', 'auxiliaries_ratio',\n",
    "    \n",
    "    # Marker linguistik\n",
    "    'marker_jadi',\n",
    "    'marker_maka_dari_itu', 'marker_menurut_saya',\n",
    "    'marker_bagi_saya', 'marker_faktanya',\n",
    "    'total_markers_count', 'markers_ratio',\n",
    "    \n",
    "    # Pola penggunaan kata\n",
    "    # 'top_first_word', 'top_first_word_freq',\n",
    "    # 'top_last_word', 'top_last_word_freq',\n",
    "    # 'favorite_2gram', 'favorite_2gram_freq',\n",
    "    # 'favorite_3gram', 'favorite_3gram_freq',\n",
    "    'double_dot_count',\n",
    "    # Fitur web/sosial media \n",
    "    'url_count', 'hashtag_count', 'mention_count'\n",
    "]\n",
    "\n",
    "# Fitur IndoBERT - fitur embedding yang dihasilkan dari model IndoBERT\n",
    "\n",
    "# Fitur statistik teks - terkait dengan ukuran statistik dari teks\n",
    "Statistik = [\n",
    "    # Statistik dasar teks\n",
    "    'word_count',\n",
    "    'ttr',  # Type-Token Ratio\n",
    "    'avg_word_length',\n",
    "    'avg_syllables_per_word',\n",
    "    'avg_word_frequency',\n",
    "    \n",
    "    # Indeks keragaman leksikal\n",
    "    'uber_index',\n",
    "    'hd_d_index',\n",
    "    'tuldava_measure_u', \n",
    "    'yules_k',\n",
    "    'shannon_entropy',\n",
    "    'simpsons_index',\n",
    "    \n",
    "    # Indeks keterbacaan\n",
    "    'flesch_kincaid',\n",
    "    'dale_chall',\n",
    "    'linsear_write',\n",
    "    'gunning_fog',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b3fe71e-e5af-461c-b07c-f46bbbf86c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comma_count</th>\n",
       "      <th>comma_ratio</th>\n",
       "      <th>period_count</th>\n",
       "      <th>period_ratio</th>\n",
       "      <th>semicolon_count</th>\n",
       "      <th>semicolon_ratio</th>\n",
       "      <th>colon_count</th>\n",
       "      <th>colon_ratio</th>\n",
       "      <th>question_mark_count</th>\n",
       "      <th>question_mark_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>marker_maka_dari_itu</th>\n",
       "      <th>marker_menurut_saya</th>\n",
       "      <th>marker_bagi_saya</th>\n",
       "      <th>marker_faktanya</th>\n",
       "      <th>total_markers_count</th>\n",
       "      <th>markers_ratio</th>\n",
       "      <th>double_dot_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14773</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14774</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14775</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14776</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14777</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14778 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       comma_count  comma_ratio  period_count  period_ratio  semicolon_count  \\\n",
       "0                0          0.0             0      0.000000                0   \n",
       "1                0          0.0             3      0.500000                0   \n",
       "2                0          0.0             0      0.000000                0   \n",
       "3                0          0.0             1      0.250000                0   \n",
       "4                0          0.0             1      0.142857                0   \n",
       "...            ...          ...           ...           ...              ...   \n",
       "14773            0          0.0             0      0.000000                0   \n",
       "14774            0          0.0             0      0.000000                0   \n",
       "14775            0          0.0             0      0.000000                0   \n",
       "14776            0          0.0             0      0.000000                0   \n",
       "14777            0          0.0             1      0.022727                0   \n",
       "\n",
       "       semicolon_ratio  colon_count  colon_ratio  question_mark_count  \\\n",
       "0                  0.0            0          0.0                    0   \n",
       "1                  0.0            0          0.0                    0   \n",
       "2                  0.0            0          0.0                    0   \n",
       "3                  0.0            0          0.0                    0   \n",
       "4                  0.0            0          0.0                    0   \n",
       "...                ...          ...          ...                  ...   \n",
       "14773              0.0            0          0.0                    0   \n",
       "14774              0.0            0          0.0                    0   \n",
       "14775              0.0            0          0.0                    0   \n",
       "14776              0.0            0          0.0                    0   \n",
       "14777              0.0            0          0.0                    0   \n",
       "\n",
       "       question_mark_ratio  ...  marker_maka_dari_itu  marker_menurut_saya  \\\n",
       "0                      0.0  ...                     0                    0   \n",
       "1                      0.0  ...                     0                    0   \n",
       "2                      0.0  ...                     0                    0   \n",
       "3                      0.0  ...                     0                    0   \n",
       "4                      0.0  ...                     0                    0   \n",
       "...                    ...  ...                   ...                  ...   \n",
       "14773                  0.0  ...                     0                    0   \n",
       "14774                  0.0  ...                     0                    0   \n",
       "14775                  0.0  ...                     0                    0   \n",
       "14776                  0.0  ...                     0                    0   \n",
       "14777                  0.0  ...                     0                    0   \n",
       "\n",
       "       marker_bagi_saya  marker_faktanya  total_markers_count  markers_ratio  \\\n",
       "0                     0                0                    0            0.0   \n",
       "1                     0                0                    0            0.0   \n",
       "2                     0                0                    0            0.0   \n",
       "3                     0                0                    0            0.0   \n",
       "4                     0                0                    0            0.0   \n",
       "...                 ...              ...                  ...            ...   \n",
       "14773                 0                0                    0            0.0   \n",
       "14774                 0                0                    0            0.0   \n",
       "14775                 0                0                    0            0.0   \n",
       "14776                 0                0                    0            0.0   \n",
       "14777                 0                0                    0            0.0   \n",
       "\n",
       "       double_dot_count  url_count  hashtag_count  mention_count  \n",
       "0                   0.0        0.0            0.0            0.0  \n",
       "1                   0.0        0.0            0.0            0.0  \n",
       "2                   0.0        0.0            0.0            0.0  \n",
       "3                   0.0        0.0            0.0            0.0  \n",
       "4                   0.0        0.0            0.0            0.0  \n",
       "...                 ...        ...            ...            ...  \n",
       "14773               0.0        0.0            0.0            0.0  \n",
       "14774               1.0        0.0            0.0            0.0  \n",
       "14775               0.0        0.0            0.0            0.0  \n",
       "14776               0.0        0.0            0.0            0.0  \n",
       "14777               0.0        0.0            0.0            0.0  \n",
       "\n",
       "[14778 rows x 58 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[stilistik]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95651f7b-6d79-46c4-8b3e-776e36aef6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitur_penting_author_attribution = [\n",
    "    # === FITUR STATISTIK TEKS (Paling Kritis) ===\n",
    "    'word_count',                    # Panjang tweet - karakteristik kuat penulis\n",
    "    'ttr',                           # Type-Token Ratio - keragaman kosakata\n",
    "    'avg_word_length',              # Rata-rata panjang kata\n",
    "    'avg_syllables_per_word',       # Kompleksitas kata\n",
    "    'shannon_entropy',              # Entropi informasi - keragaman linguistik\n",
    "    'yules_k',                      # Ukuran keragaman leksikal yang robust\n",
    "\n",
    "    # === FITUR TANDA BACA (Sangat Kritis untuk Tweet) ===\n",
    "    'comma_ratio',                  # Penggunaan koma relatif\n",
    "    'period_ratio',                 # Penggunaan titik relatif\n",
    "    'exclamation_mark_ratio',       # Emosi/penekanan - khas untuk tweet\n",
    "    'question_mark_ratio',          # Gaya bertanya\n",
    "    'ellipsis_count',               # Titik tiga - gaya informal\n",
    "    'quotation_count',              # Penggunaan kutip\n",
    "    'total_punct_ratio',            # Total penggunaan tanda baca\n",
    "    'punct_diversity',              # Keragaman tanda baca\n",
    "\n",
    "    # === FITUR JENIS KATA (Penting untuk Gaya Bahasa) ===\n",
    "    'function_ratio',               # Rasio kata fungsi\n",
    "    'content_ratio',                # Rasio kata konten\n",
    "    'function_to_content_ratio',    # Perbandingan kata fungsi vs konten\n",
    "    'pronouns_ratio',               # Penggunaan kata ganti - sangat personal\n",
    "    'conjunctions_ratio',           # Gaya menghubungkan ide\n",
    "    'prepositions_ratio',           # Kompleksitas struktur kalimat\n",
    "\n",
    "    # === FITUR MARKER LINGUISTIK (Spesifik Bahasa Indonesia) ===\n",
    "    'marker_jadi',                  # Marker \"jadi\" - khas informal Indonesia\n",
    "    'marker_menurut_saya',          # Ekspresi opini personal\n",
    "    'marker_bagi_saya',             # Ekspresi perspektif personal\n",
    "    'marker_faktanya',              # Marker argumentasi\n",
    "    'total_markers_count',          # Total marker linguistik\n",
    "    'markers_ratio',                # Rasio penggunaan marker\n",
    "\n",
    "    # === FITUR MEDIA SOSIAL (Esensial untuk Tweet) ===\n",
    "    'hashtag_count',                # Penggunaan hashtag\n",
    "    'mention_count',                # Penggunaan mention (@)\n",
    "    'url_count',                    # Berbagi link\n",
    "\n",
    "    # === FITUR TAMBAHAN YANG RELEVAN ===\n",
    "    'double_dot_count',             # Penggunaan titik ganda (..)\n",
    "    'avg_word_frequency',           # Penggunaan kata umum vs jarang\n",
    "\n",
    "    # === FITUR N-GRAM DAN STRUKTUR KATA TAMBAHAN ===\n",
    "    'char_2gram_freq_top10',        # Frekuensi 10 karakter bigram teratas\n",
    "    'char_3gram_freq_top10',        # Frekuensi 10 karakter trigram teratas  \n",
    "    'vowel_consonant_ratio',        # Rasio vokal dan konsonan\n",
    "    'reduplication_count',          # Jumlah kata ulang\n",
    "    'prefix_count',                 # Jumlah prefiks\n",
    "    'suffix_count'                  # Jumlah sufiks\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72155c6b-21e6-425c-a907-90d0e6949eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_columns = [\n",
    "    'conversation_id_str',\n",
    "    'created_at',\n",
    "    'favorite_count',\n",
    "    'full_text',\n",
    "    'id_str',\n",
    "    'image_url',\n",
    "    'in_reply_to_screen_name',\n",
    "    'lang',\n",
    "    'location',\n",
    "    'quote_count',\n",
    "    'reply_count',\n",
    "    'retweet_count',\n",
    "    'tweet_url',\n",
    "    'user_id_str',\n",
    "    'username',\n",
    "    'full_text_clean',\n",
    "    'full_text_clean_2'\n",
    "]\n",
    "\n",
    "# Menambahkan fitur IndoBERT (1-80)\n",
    "indobert_features = [f'indobert_feature_{i}' for i in range(1, 81)]\n",
    "\n",
    "# Menggabungkan semuanya dalam satu list\n",
    "all_columns = tweet_columns + indobert_features\n",
    "df_indobert = df[all_columns]\n",
    "\n",
    "df_indobert.to_csv(\"../data/data_fix_indobert.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9dcfc-25d0-43b4-9a85-d346d22b6db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id_str</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>id_str</th>\n",
       "      <th>image_url</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>indobert_feature_71</th>\n",
       "      <th>indobert_feature_72</th>\n",
       "      <th>indobert_feature_73</th>\n",
       "      <th>indobert_feature_74</th>\n",
       "      <th>indobert_feature_75</th>\n",
       "      <th>indobert_feature_76</th>\n",
       "      <th>indobert_feature_77</th>\n",
       "      <th>indobert_feature_78</th>\n",
       "      <th>indobert_feature_79</th>\n",
       "      <th>indobert_feature_80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.538779e+18</td>\n",
       "      <td>2022-06-20 07:01:14+00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>#JokowiMatikanDemokrasi #JokowiMatikanDemokrasi</td>\n",
       "      <td>1.538779e+18</td>\n",
       "      <td>https://pbs.twimg.com/media/FVrXVWnUYAEBDfI.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312094</td>\n",
       "      <td>0.980298</td>\n",
       "      <td>-0.497925</td>\n",
       "      <td>1.602446</td>\n",
       "      <td>-0.254567</td>\n",
       "      <td>-0.520526</td>\n",
       "      <td>-0.353177</td>\n",
       "      <td>-1.112838</td>\n",
       "      <td>-0.001756</td>\n",
       "      <td>-0.027748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.538783e+18</td>\n",
       "      <td>2022-06-20 07:16:30+00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Polisi kalah sama lont... #JokowiMatikanDemokr...</td>\n",
       "      <td>1.538783e+18</td>\n",
       "      <td>https://pbs.twimg.com/media/FVra02UUAAAJ4-R.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.409329</td>\n",
       "      <td>0.674773</td>\n",
       "      <td>-0.127567</td>\n",
       "      <td>0.804900</td>\n",
       "      <td>-0.329457</td>\n",
       "      <td>-0.290906</td>\n",
       "      <td>-0.106401</td>\n",
       "      <td>0.068177</td>\n",
       "      <td>0.417723</td>\n",
       "      <td>-0.022249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.538783e+18</td>\n",
       "      <td>2022-06-20 07:17:55+00:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Setuju #JokowiMatikanDemokrasi #JokowiMatikanD...</td>\n",
       "      <td>1.538783e+18</td>\n",
       "      <td>https://pbs.twimg.com/media/FVrbJuBUYAAbmJH.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204514</td>\n",
       "      <td>0.745370</td>\n",
       "      <td>-0.635906</td>\n",
       "      <td>1.242655</td>\n",
       "      <td>-0.276266</td>\n",
       "      <td>-0.039602</td>\n",
       "      <td>-0.501906</td>\n",
       "      <td>-1.205206</td>\n",
       "      <td>0.055984</td>\n",
       "      <td>-0.102962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.538783e+18</td>\n",
       "      <td>2022-06-20 07:18:41+00:00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Bubarkan KPK. #JokowiMatikanDemokrasi #JokowiM...</td>\n",
       "      <td>1.538783e+18</td>\n",
       "      <td>https://pbs.twimg.com/media/FVrbU4oUUAE3aS8.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228128</td>\n",
       "      <td>0.552171</td>\n",
       "      <td>-0.218623</td>\n",
       "      <td>1.189514</td>\n",
       "      <td>-0.291000</td>\n",
       "      <td>-0.466457</td>\n",
       "      <td>-0.305309</td>\n",
       "      <td>-0.516276</td>\n",
       "      <td>0.302189</td>\n",
       "      <td>0.132921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.538786e+18</td>\n",
       "      <td>2022-06-20 07:27:45+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rakyat maunya jokowi legowo MUNDUR. #JokowiMat...</td>\n",
       "      <td>1.538786e+18</td>\n",
       "      <td>https://pbs.twimg.com/media/FVrdZ7iUUAQBg2M.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081445</td>\n",
       "      <td>0.669836</td>\n",
       "      <td>-0.144411</td>\n",
       "      <td>0.584860</td>\n",
       "      <td>-0.471251</td>\n",
       "      <td>-0.212964</td>\n",
       "      <td>-0.327029</td>\n",
       "      <td>-0.617646</td>\n",
       "      <td>0.011081</td>\n",
       "      <td>0.291943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   conversation_id_str                 created_at  favorite_count  \\\n",
       "0         1.538779e+18  2022-06-20 07:01:14+00:00             5.0   \n",
       "1         1.538783e+18  2022-06-20 07:16:30+00:00             4.0   \n",
       "2         1.538783e+18  2022-06-20 07:17:55+00:00            10.0   \n",
       "3         1.538783e+18  2022-06-20 07:18:41+00:00            20.0   \n",
       "4         1.538786e+18  2022-06-20 07:27:45+00:00             0.0   \n",
       "\n",
       "                                           full_text        id_str  \\\n",
       "0    #JokowiMatikanDemokrasi #JokowiMatikanDemokrasi  1.538779e+18   \n",
       "1  Polisi kalah sama lont... #JokowiMatikanDemokr...  1.538783e+18   \n",
       "2  Setuju #JokowiMatikanDemokrasi #JokowiMatikanD...  1.538783e+18   \n",
       "3  Bubarkan KPK. #JokowiMatikanDemokrasi #JokowiM...  1.538783e+18   \n",
       "4  Rakyat maunya jokowi legowo MUNDUR. #JokowiMat...  1.538786e+18   \n",
       "\n",
       "                                         image_url in_reply_to_screen_name  \\\n",
       "0  https://pbs.twimg.com/media/FVrXVWnUYAEBDfI.jpg                     NaN   \n",
       "1  https://pbs.twimg.com/media/FVra02UUAAAJ4-R.jpg                     NaN   \n",
       "2  https://pbs.twimg.com/media/FVrbJuBUYAAbmJH.jpg                     NaN   \n",
       "3  https://pbs.twimg.com/media/FVrbU4oUUAE3aS8.jpg                     NaN   \n",
       "4  https://pbs.twimg.com/media/FVrdZ7iUUAQBg2M.jpg                     NaN   \n",
       "\n",
       "  lang location  quote_count  ...  indobert_feature_71  indobert_feature_72  \\\n",
       "0  qme      NaN          0.0  ...            -0.312094             0.980298   \n",
       "1   in      NaN          0.0  ...            -0.409329             0.674773   \n",
       "2   in      NaN          0.0  ...            -0.204514             0.745370   \n",
       "3   in      NaN          3.0  ...            -0.228128             0.552171   \n",
       "4   in      NaN          0.0  ...            -0.081445             0.669836   \n",
       "\n",
       "  indobert_feature_73  indobert_feature_74 indobert_feature_75  \\\n",
       "0           -0.497925             1.602446           -0.254567   \n",
       "1           -0.127567             0.804900           -0.329457   \n",
       "2           -0.635906             1.242655           -0.276266   \n",
       "3           -0.218623             1.189514           -0.291000   \n",
       "4           -0.144411             0.584860           -0.471251   \n",
       "\n",
       "  indobert_feature_76 indobert_feature_77  indobert_feature_78  \\\n",
       "0           -0.520526           -0.353177            -1.112838   \n",
       "1           -0.290906           -0.106401             0.068177   \n",
       "2           -0.039602           -0.501906            -1.205206   \n",
       "3           -0.466457           -0.305309            -0.516276   \n",
       "4           -0.212964           -0.327029            -0.617646   \n",
       "\n",
       "   indobert_feature_79  indobert_feature_80  \n",
       "0            -0.001756            -0.027748  \n",
       "1             0.417723            -0.022249  \n",
       "2             0.055984            -0.102962  \n",
       "3             0.302189             0.132921  \n",
       "4             0.011081             0.291943  \n",
       "\n",
       "[5 rows x 97 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/data_fix_indobert.csv\")\n",
    "df.head()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from math import log2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def extract_author_attribution_features(df):\n",
    "    \"\"\"\n",
    "    Ekstraksi fitur author attribution untuk tweet berbahasa Indonesia\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame dengan kolom 'full_text' dan 'username'\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame dengan fitur tambahan untuk author attribution\n",
    "    \"\"\"\n",
    "    \n",
    "    # Copy dataframe untuk tidak mengubah original\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Daftar kata fungsi bahasa Indonesia (function words)\n",
    "    function_words = {\n",
    "        'dan', 'atau', 'tetapi', 'namun', 'tapi', 'serta', 'atau', 'maupun',\n",
    "        'di', 'ke', 'dari', 'pada', 'dalam', 'untuk', 'dengan', 'oleh', 'sebagai',\n",
    "        'yang', 'ini', 'itu', 'tersebut', 'tersebut', 'ini', 'itu',\n",
    "        'saya', 'aku', 'kamu', 'anda', 'dia', 'mereka', 'kita', 'kami',\n",
    "        'adalah', 'akan', 'telah', 'sudah', 'sedang', 'masih', 'pernah',\n",
    "        'tidak', 'bukan', 'jangan', 'belum', 'tak', 'ga', 'gak', 'nggak',\n",
    "        'ya', 'iya', 'yah', 'sih', 'kok', 'kan', 'lah', 'kah', 'tah',\n",
    "        'jika', 'kalau', 'bila', 'apabila', 'ketika', 'saat', 'waktu',\n",
    "        'karena', 'sebab', 'akibat', 'maka', 'sehingga', 'supaya', 'agar'\n",
    "    }\n",
    "    \n",
    "    # Daftar kata ganti (pronouns)\n",
    "    pronouns = {'saya', 'aku', 'kamu', 'anda', 'dia', 'mereka', 'kita', 'kami', 'kalian'}\n",
    "    \n",
    "    # Daftar kata sambung (conjunctions)\n",
    "    conjunctions = {'dan', 'atau', 'tetapi', 'namun', 'tapi', 'serta', 'maupun', 'bahkan', 'malahan'}\n",
    "    \n",
    "    # Daftar kata depan (prepositions)\n",
    "    prepositions = {'di', 'ke', 'dari', 'pada', 'dalam', 'untuk', 'dengan', 'oleh', 'sebagai', 'antara', 'hingga'}\n",
    "    \n",
    "    # Membuat kamus frekuensi kata untuk avg_word_frequency\n",
    "    all_words = []\n",
    "    for text in df_features['full_text']:\n",
    "        if pd.notna(text):\n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            all_words.extend(words)\n",
    "    \n",
    "    word_freq = Counter(all_words)\n",
    "    total_words = len(all_words)\n",
    "    \n",
    "    # === FITUR TAMBAHAN UNTUK BAHASA INDONESIA ===\n",
    "    # Definisi huruf vokal dan konsonan untuk bahasa Indonesia\n",
    "    vowels = set('aiueoAIUEO')\n",
    "    consonants = set('bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ')\n",
    "    \n",
    "    # Daftar prefiks dan sufiks umum dalam bahasa Indonesia\n",
    "    common_prefixes = [\n",
    "        'ber', 'ter', 'per', 'me', 'mem', 'men', 'meng', 'meny', 'ke', 'se', \n",
    "        'di', 'pen', 'peng', 'peny', 'pel', 'per', 'pre', 'pro', 'anti', 'eks'\n",
    "    ]\n",
    "    \n",
    "    common_suffixes = [\n",
    "        'an', 'kan', 'nya', 'lah', 'kah', 'tah', 'pun', 'ku', 'mu', 'ing',\n",
    "        'isme', 'is', 'ik', 'al', 'er', 'or', 'si', 'ti', 'wi', 'man'\n",
    "    ]\n",
    "    \n",
    "    # Fungsi helper untuk mengekstrak n-gram karakter\n",
    "    def get_char_ngrams(text, n):\n",
    "        \"\"\"Mengekstrak n-gram karakter dari teks\"\"\"\n",
    "        text = text.lower()\n",
    "        # Hanya ambil karakter alfabet dan spasi\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        ngrams = []\n",
    "        for i in range(len(text) - n + 1):\n",
    "            ngram = text[i:i+n]\n",
    "            if not ngram.isspace():  # Skip n-gram yang hanya spasi\n",
    "                ngrams.append(ngram)\n",
    "        return ngrams\n",
    "    \n",
    "    # Fungsi untuk menghitung rasio vokal-konsonan\n",
    "    def vowel_consonant_ratio(text):\n",
    "        \"\"\"Menghitung rasio vokal terhadap konsonan\"\"\"\n",
    "        vowel_count = sum(1 for char in text if char in vowels)\n",
    "        consonant_count = sum(1 for char in text if char in consonants)\n",
    "        \n",
    "        if consonant_count == 0:\n",
    "            return vowel_count if vowel_count > 0 else 0\n",
    "        return vowel_count / consonant_count\n",
    "    \n",
    "    # Fungsi untuk menghitung reduplikasi\n",
    "    def count_reduplication(text):\n",
    "        \"\"\"Menghitung jumlah kata yang mengandung reduplikasi\"\"\"\n",
    "        words = text.lower().split()\n",
    "        reduplication_count = 0\n",
    "        \n",
    "        for word in words:\n",
    "            # Cek reduplikasi penuh (kata-kata)\n",
    "            if '-' in word:\n",
    "                parts = word.split('-')\n",
    "                if len(parts) == 2 and parts[0] == parts[1]:\n",
    "                    reduplication_count += 1\n",
    "            \n",
    "            # Cek reduplikasi parsial (mengulang sebagian kata)\n",
    "            # Contoh: bebek, memem, gegap\n",
    "            for i in range(1, len(word)//2 + 1):\n",
    "                if word[:i] == word[i:2*i] and len(word[:i]) >= 2:\n",
    "                    reduplication_count += 1\n",
    "                    break\n",
    "                    \n",
    "        return reduplication_count\n",
    "    \n",
    "    # Fungsi untuk menghitung prefiks\n",
    "    def count_prefixes(text):\n",
    "        \"\"\"Menghitung jumlah kata yang mengandung prefiks\"\"\"\n",
    "        words = text.lower().split()\n",
    "        prefix_count = 0\n",
    "        \n",
    "        for word in words:\n",
    "            for prefix in common_prefixes:\n",
    "                if word.startswith(prefix) and len(word) > len(prefix):\n",
    "                    prefix_count += 1\n",
    "                    break\n",
    "                    \n",
    "        return prefix_count\n",
    "    \n",
    "    # Fungsi untuk menghitung sufiks\n",
    "    def count_suffixes(text):\n",
    "        \"\"\"Menghitung jumlah kata yang mengandung sufiks\"\"\"\n",
    "        words = text.lower().split()\n",
    "        suffix_count = 0\n",
    "        \n",
    "        for word in words:\n",
    "            for suffix in common_suffixes:\n",
    "                if word.endswith(suffix) and len(word) > len(suffix):\n",
    "                    suffix_count += 1\n",
    "                    break\n",
    "                    \n",
    "        return suffix_count\n",
    "    \n",
    "    # Ekstraksi n-gram untuk mendapatkan yang paling frequent\n",
    "    print(\"Menganalisis n-gram karakter...\")\n",
    "    char_2grams_all = []\n",
    "    char_3grams_all = []\n",
    "    \n",
    "    # Kumpulkan semua n-gram untuk mendapatkan yang paling frequent\n",
    "    for text in df_features['full_text']:\n",
    "        if pd.notna(text):\n",
    "            char_2grams_all.extend(get_char_ngrams(str(text), 2))\n",
    "            char_3grams_all.extend(get_char_ngrams(str(text), 3))\n",
    "    \n",
    "    # Dapatkan 10 teratas untuk 2-gram dan 3-gram\n",
    "    top_2grams = [item[0] for item in Counter(char_2grams_all).most_common(10)]\n",
    "    top_3grams = [item[0] for item in Counter(char_3grams_all).most_common(10)]\n",
    "    \n",
    "    print(f\"Top 10 2-grams: {top_2grams}\")\n",
    "    print(f\"Top 10 3-grams: {top_3grams}\")\n",
    "    \n",
    "    def extract_features_from_text(text):\n",
    "        \"\"\"Ekstraksi fitur dari satu teks\"\"\"\n",
    "        if pd.isna(text) or text.strip() == '':\n",
    "            return {feature: 0 for feature in fitur_penting_author_attribution}\n",
    "        \n",
    "        features = {}\n",
    "        text = str(text)\n",
    "        \n",
    "        # Preprocessing\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        # === FITUR STATISTIK TEKS ===\n",
    "        features['word_count'] = len(words)\n",
    "        \n",
    "        # Type-Token Ratio (TTR)\n",
    "        unique_words = len(set(words))\n",
    "        features['ttr'] = unique_words / len(words) if words else 0\n",
    "        \n",
    "        # Rata-rata panjang kata\n",
    "        features['avg_word_length'] = sum(len(word) for word in words) / len(words) if words else 0\n",
    "        \n",
    "        # Rata-rata suku kata per kata (estimasi sederhana)\n",
    "        def count_syllables(word):\n",
    "            vowels = 'aiueo'\n",
    "            count = sum(1 for char in word.lower() if char in vowels)\n",
    "            return max(1, count)\n",
    "        \n",
    "        features['avg_syllables_per_word'] = sum(count_syllables(word) for word in words) / len(words) if words else 0\n",
    "        \n",
    "        # Shannon Entropy\n",
    "        if words:\n",
    "            word_counts = Counter(words)\n",
    "            total = len(words)\n",
    "            entropy = -sum((count/total) * log2(count/total) for count in word_counts.values())\n",
    "            features['shannon_entropy'] = entropy\n",
    "        else:\n",
    "            features['shannon_entropy'] = 0\n",
    "        \n",
    "        # Yule's K\n",
    "        if words:\n",
    "            word_counts = Counter(words)\n",
    "            freq_freqs = Counter(word_counts.values())\n",
    "            N = len(words)\n",
    "            try:\n",
    "                yules_k = 10000 * (sum(freq * (freq_count**2) for freq, freq_count in freq_freqs.items()) - N) / (N**2)\n",
    "                features['yules_k'] = yules_k\n",
    "            except:\n",
    "                features['yules_k'] = 0\n",
    "        else:\n",
    "            features['yules_k'] = 0\n",
    "        \n",
    "        # === FITUR TANDA BACA ===\n",
    "        punct_counts = {\n",
    "            ',': text.count(','),\n",
    "            '.': text.count('.'),\n",
    "            '!': text.count('!'),\n",
    "            '?': text.count('?'),\n",
    "            '...': text.count('...'),\n",
    "            '\"': text.count('\"') + text.count(\"'\"),\n",
    "        }\n",
    "        \n",
    "        total_chars = len(text)\n",
    "        features['comma_ratio'] = punct_counts[','] / total_chars if total_chars > 0 else 0\n",
    "        features['period_ratio'] = punct_counts['.'] / total_chars if total_chars > 0 else 0\n",
    "        features['exclamation_mark_ratio'] = punct_counts['!'] / total_chars if total_chars > 0 else 0\n",
    "        features['question_mark_ratio'] = punct_counts['?'] / total_chars if total_chars > 0 else 0\n",
    "        features['ellipsis_count'] = punct_counts['...']\n",
    "        features['quotation_count'] = punct_counts['\"']\n",
    "        \n",
    "        # Total tanda baca\n",
    "        all_punct = re.findall(r'[^\\w\\s]', text)\n",
    "        features['total_punct_ratio'] = len(all_punct) / total_chars if total_chars > 0 else 0\n",
    "        \n",
    "        # Keragaman tanda baca\n",
    "        unique_punct = len(set(all_punct))\n",
    "        features['punct_diversity'] = unique_punct / len(all_punct) if all_punct else 0\n",
    "        \n",
    "        # === FITUR JENIS KATA ===\n",
    "        function_word_count = sum(1 for word in words if word in function_words)\n",
    "        content_word_count = len(words) - function_word_count\n",
    "        \n",
    "        features['function_ratio'] = function_word_count / len(words) if words else 0\n",
    "        features['content_ratio'] = content_word_count / len(words) if words else 0\n",
    "        features['function_to_content_ratio'] = function_word_count / content_word_count if content_word_count > 0 else 0\n",
    "        \n",
    "        # Kata ganti\n",
    "        pronoun_count = sum(1 for word in words if word in pronouns)\n",
    "        features['pronouns_ratio'] = pronoun_count / len(words) if words else 0\n",
    "        \n",
    "        # Kata sambung\n",
    "        conjunction_count = sum(1 for word in words if word in conjunctions)\n",
    "        features['conjunctions_ratio'] = conjunction_count / len(words) if words else 0\n",
    "        \n",
    "        # Kata depan\n",
    "        preposition_count = sum(1 for word in words if word in prepositions)\n",
    "        features['prepositions_ratio'] = preposition_count / len(words) if words else 0\n",
    "        \n",
    "        # === FITUR MARKER LINGUISTIK ===\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        features['marker_jadi'] = text_lower.count('jadi')\n",
    "        features['marker_menurut_saya'] = text_lower.count('menurut saya') + text_lower.count('menurut aku')\n",
    "        features['marker_bagi_saya'] = text_lower.count('bagi saya') + text_lower.count('bagi aku')\n",
    "        features['marker_faktanya'] = text_lower.count('faktanya') + text_lower.count('kenyataannya')\n",
    "        \n",
    "        total_markers = (features['marker_jadi'] + features['marker_menurut_saya'] + \n",
    "                        features['marker_bagi_saya'] + features['marker_faktanya'])\n",
    "        features['total_markers_count'] = total_markers\n",
    "        features['markers_ratio'] = total_markers / len(words) if words else 0\n",
    "        \n",
    "        # === FITUR MEDIA SOSIAL ===\n",
    "        features['hashtag_count'] = len(re.findall(r'#\\w+', text))\n",
    "        features['mention_count'] = len(re.findall(r'@\\w+', text))\n",
    "        features['url_count'] = len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n",
    "        \n",
    "        # === FITUR TAMBAHAN ===\n",
    "        features['double_dot_count'] = text.count('..')\n",
    "        \n",
    "        # Rata-rata frekuensi kata\n",
    "        if words:\n",
    "            word_frequencies = [word_freq.get(word, 0) / total_words for word in words]\n",
    "            features['avg_word_frequency'] = sum(word_frequencies) / len(word_frequencies)\n",
    "        else:\n",
    "            features['avg_word_frequency'] = 0\n",
    "        \n",
    "        # === FITUR TAMBAHAN BAHASA INDONESIA ===\n",
    "        # Character n-grams\n",
    "        text_2grams = get_char_ngrams(text, 2)\n",
    "        text_2gram_counter = Counter(text_2grams)\n",
    "        \n",
    "        for i, gram in enumerate(top_2grams):\n",
    "            features[f'char_2gram_freq_top10_{i+1}'] = text_2gram_counter.get(gram, 0)\n",
    "        \n",
    "        text_3grams = get_char_ngrams(text, 3)\n",
    "        text_3gram_counter = Counter(text_3grams)\n",
    "        \n",
    "        for i, gram in enumerate(top_3grams):\n",
    "            features[f'char_3gram_freq_top10_{i+1}'] = text_3gram_counter.get(gram, 0)\n",
    "        \n",
    "        # Fitur morfologi Indonesia\n",
    "        features['vowel_consonant_ratio'] = vowel_consonant_ratio(text)\n",
    "        features['reduplication_count'] = count_reduplication(text)\n",
    "        features['prefix_count'] = count_prefixes(text)\n",
    "        features['suffix_count'] = count_suffixes(text)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    # Ekstraksi fitur untuk setiap baris\n",
    "    print(\"Mengekstrak fitur author attribution...\")\n",
    "    \n",
    "    # Inisialisasi kolom fitur\n",
    "    for feature in fitur_penting_author_attribution:\n",
    "        df_features[feature] = 0.0\n",
    "    \n",
    "    # Inisialisasi kolom untuk n-gram features\n",
    "    for i in range(10):\n",
    "        df_features[f'char_2gram_freq_top10_{i+1}'] = 0\n",
    "        df_features[f'char_3gram_freq_top10_{i+1}'] = 0\n",
    "    \n",
    "    # Inisialisasi kolom untuk fitur bahasa Indonesia\n",
    "    df_features['vowel_consonant_ratio'] = 0.0\n",
    "    df_features['reduplication_count'] = 0\n",
    "    df_features['prefix_count'] = 0\n",
    "    df_features['suffix_count'] = 0\n",
    "    \n",
    "    # Ekstraksi fitur baris per baris\n",
    "    for idx, row in df_features.iterrows():\n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Memproses baris {idx+1}/{len(df_features)}\")\n",
    "        \n",
    "        text = row['full_text']\n",
    "        features = extract_features_from_text(text)\n",
    "        \n",
    "        # Assign fitur ke dataframe\n",
    "        for feature, value in features.items():\n",
    "            df_features.at[idx, feature] = value\n",
    "    \n",
    "    print(\"Ekstraksi fitur selesai!\")\n",
    "    return df_features\n",
    "\n",
    "# Daftar fitur yang akan diekstrak\n",
    "fitur_penting_author_attribution = [\n",
    "    # === FITUR STATISTIK TEKS (Paling Kritis) ===\n",
    "    'word_count',                    # Panjang tweet - karakteristik kuat penulis\n",
    "    'ttr',                           # Type-Token Ratio - keragaman kosakata\n",
    "    'avg_word_length',              # Rata-rata panjang kata\n",
    "    'avg_syllables_per_word',       # Kompleksitas kata\n",
    "    'shannon_entropy',              # Entropi informasi - keragaman linguistik\n",
    "    'yules_k',                      # Ukuran keragaman leksikal yang robust\n",
    "    # === FITUR TANDA BACA (Sangat Kritis untuk Tweet) ===\n",
    "    'comma_ratio',                  # Penggunaan koma relatif\n",
    "    'period_ratio',                 # Penggunaan titik relatif\n",
    "    'exclamation_mark_ratio',       # Emosi/penekanan - khas untuk tweet\n",
    "    'question_mark_ratio',          # Gaya bertanya\n",
    "    'ellipsis_count',               # Titik tiga - gaya informal\n",
    "    'quotation_count',              # Penggunaan kutip\n",
    "    'total_punct_ratio',            # Total penggunaan tanda baca\n",
    "    'punct_diversity',              # Keragaman tanda baca\n",
    "    # === FITUR JENIS KATA (Penting untuk Gaya Bahasa) ===\n",
    "    'function_ratio',               # Rasio kata fungsi\n",
    "    'content_ratio',                # Rasio kata konten\n",
    "    'function_to_content_ratio',    # Perbandingan kata fungsi vs konten\n",
    "    'pronouns_ratio',               # Penggunaan kata ganti - sangat personal\n",
    "    'conjunctions_ratio',           # Gaya menghubungkan ide\n",
    "    'prepositions_ratio',           # Kompleksitas struktur kalimat\n",
    "    # === FITUR MARKER LINGUISTIK (Spesifik Bahasa Indonesia) ===\n",
    "    'marker_jadi',                  # Marker \"jadi\" - khas informal Indonesia\n",
    "    'marker_menurut_saya',          # Ekspresi opini personal\n",
    "    'marker_bagi_saya',             # Ekspresi perspektif personal\n",
    "    'marker_faktanya',              # Marker argumentasi\n",
    "    'total_markers_count',          # Total marker linguistik\n",
    "    'markers_ratio',                # Rasio penggunaan marker\n",
    "    # === FITUR MEDIA SOSIAL (Esensial untuk Tweet) ===\n",
    "    'hashtag_count',                # Penggunaan hashtag\n",
    "    'mention_count',                # Penggunaan mention (@)\n",
    "    'url_count',                    # Berbagi link\n",
    "    # === FITUR TAMBAHAN YANG RELEVAN ===\n",
    "    'double_dot_count',             # Penggunaan titik ganda (..)\n",
    "    'avg_word_frequency',           # Penggunaan kata umum vs jarang\n",
    "]\n",
    "\n",
    "# Fitur tambahan untuk bahasa Indonesia\n",
    "fitur_bahasa_indonesia = [\n",
    "    'char_2gram_freq_top10_1', 'char_2gram_freq_top10_2', 'char_2gram_freq_top10_3', 'char_2gram_freq_top10_4', 'char_2gram_freq_top10_5',\n",
    "    'char_2gram_freq_top10_6', 'char_2gram_freq_top10_7', 'char_2gram_freq_top10_8', 'char_2gram_freq_top10_9', 'char_2gram_freq_top10_10',\n",
    "    'char_3gram_freq_top10_1', 'char_3gram_freq_top10_2', 'char_3gram_freq_top10_3', 'char_3gram_freq_top10_4', 'char_3gram_freq_top10_5',\n",
    "    'char_3gram_freq_top10_6', 'char_3gram_freq_top10_7', 'char_3gram_freq_top10_8', 'char_3gram_freq_top10_9', 'char_3gram_freq_top10_10',\n",
    "    'vowel_consonant_ratio',\n",
    "    'reduplication_count',\n",
    "    'prefix_count',\n",
    "    'suffix_count'\n",
    "]\n",
    "\n",
    "# Gabungan semua fitur\n",
    "all_features = fitur_penting_author_attribution + fitur_bahasa_indonesia\n",
    "\n",
    "# Contoh penggunaan:\n",
    "\"\"\"\n",
    "# Assuming you have a dataframe 'df' with 'full_text' and 'username' columns\n",
    "df_with_features = extract_author_attribution_features(df)\n",
    "\n",
    "# Melihat semua fitur yang telah diekstrak\n",
    "print(\"Fitur statistik dasar:\")\n",
    "print(df_with_features[fitur_penting_author_attribution].head())\n",
    "\n",
    "print(\"\\nFitur bahasa Indonesia:\")\n",
    "print(df_with_features[fitur_bahasa_indonesia].head())\n",
    "\n",
    "# Statistik dasar semua fitur\n",
    "print(\"\\nStatistik dasar semua fitur:\")\n",
    "print(df_with_features[all_features].describe())\n",
    "\n",
    "# Melihat top n-grams yang digunakan\n",
    "print(\"\\nTop 10 Character 2-grams yang dianalisis:\")\n",
    "for i in range(10):\n",
    "    col_name = f'char_2gram_freq_top10_{i+1}'\n",
    "    if col_name in df_with_features.columns:\n",
    "        print(f\"Position {i+1}: {col_name}\")\n",
    "\n",
    "print(\"\\nTop 10 Character 3-grams yang dianalisis:\")\n",
    "for i in range(10):\n",
    "    col_name = f'char_3gram_freq_top10_{i+1}'\n",
    "    if col_name in df_with_features.columns:\n",
    "        print(f\"Position {i+1}: {col_name}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "734ee46e-e493-496a-87c9-e1310758e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from math import log2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def extract_author_attribution_features(df):\n",
    "    \"\"\"\n",
    "    Ekstraksi fitur author attribution untuk tweet berbahasa Indonesia\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame dengan kolom 'full_text' dan 'username'\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame dengan fitur tambahan untuk author attribution\n",
    "    \"\"\"\n",
    "    \n",
    "    # Copy dataframe untuk tidak mengubah original\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Daftar kata fungsi bahasa Indonesia (function words)\n",
    "    function_words = {\n",
    "        'dan', 'atau', 'tetapi', 'namun', 'tapi', 'serta', 'atau', 'maupun',\n",
    "        'di', 'ke', 'dari', 'pada', 'dalam', 'untuk', 'dengan', 'oleh', 'sebagai',\n",
    "        'yang', 'ini', 'itu', 'tersebut', 'tersebut', 'ini', 'itu',\n",
    "        'saya', 'aku', 'kamu', 'anda', 'dia', 'mereka', 'kita', 'kami',\n",
    "        'adalah', 'akan', 'telah', 'sudah', 'sedang', 'masih', 'pernah',\n",
    "        'tidak', 'bukan', 'jangan', 'belum', 'tak', 'ga', 'gak', 'nggak',\n",
    "        'ya', 'iya', 'yah', 'sih', 'kok', 'kan', 'lah', 'kah', 'tah',\n",
    "        'jika', 'kalau', 'bila', 'apabila', 'ketika', 'saat', 'waktu',\n",
    "        'karena', 'sebab', 'akibat', 'maka', 'sehingga', 'supaya', 'agar'\n",
    "    }\n",
    "    \n",
    "    # Daftar kata ganti (pronouns)\n",
    "    pronouns = {'saya', 'aku', 'kamu', 'anda', 'dia', 'mereka', 'kita', 'kami', 'kalian'}\n",
    "    \n",
    "    # Daftar kata sambung (conjunctions)\n",
    "    conjunctions = {'dan', 'atau', 'tetapi', 'namun', 'tapi', 'serta', 'maupun', 'bahkan', 'malahan'}\n",
    "    \n",
    "    # Daftar kata depan (prepositions)\n",
    "    prepositions = {'di', 'ke', 'dari', 'pada', 'dalam', 'untuk', 'dengan', 'oleh', 'sebagai', 'antara', 'hingga'}\n",
    "    \n",
    "    # Membuat kamus frekuensi kata untuk avg_word_frequency\n",
    "    all_words = []\n",
    "    for text in df_features['full_text']:\n",
    "        if pd.notna(text):\n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            all_words.extend(words)\n",
    "    \n",
    "    word_freq = Counter(all_words)\n",
    "    total_words = len(all_words)\n",
    "    \n",
    "    # === FITUR TAMBAHAN UNTUK BAHASA INDONESIA ===\n",
    "    # Definisi huruf vokal dan konsonan untuk bahasa Indonesia\n",
    "    vowels = set('aiueoAIUEO')\n",
    "    consonants = set('bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ')\n",
    "    \n",
    "    # Daftar prefiks dan sufiks umum dalam bahasa Indonesia\n",
    "    common_prefixes = [\n",
    "        'ber', 'ter', 'per', 'me', 'mem', 'men', 'meng', 'meny', 'ke', 'se', \n",
    "        'di', 'pen', 'peng', 'peny', 'pel', 'per', 'pre', 'pro', 'anti', 'eks'\n",
    "    ]\n",
    "    \n",
    "    common_suffixes = [\n",
    "        'an', 'kan', 'nya', 'lah', 'kah', 'tah', 'pun', 'ku', 'mu', 'ing',\n",
    "        'isme', 'is', 'ik', 'al', 'er', 'or', 'si', 'ti', 'wi', 'man'\n",
    "    ]\n",
    "    \n",
    "    # Fungsi helper untuk mengekstrak n-gram karakter\n",
    "    def get_char_ngrams(text, n):\n",
    "        \"\"\"Mengekstrak n-gram karakter dari teks\"\"\"\n",
    "        text = text.lower()\n",
    "        # Hanya ambil karakter alfabet dan spasi\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        ngrams = []\n",
    "        for i in range(len(text) - n + 1):\n",
    "            ngram = text[i:i+n]\n",
    "            if not ngram.isspace():  # Skip n-gram yang hanya spasi\n",
    "                ngrams.append(ngram)\n",
    "        return ngrams\n",
    "    \n",
    "    # Fungsi untuk menghitung rasio vokal-konsonan\n",
    "    def vowel_consonant_ratio(text):\n",
    "        \"\"\"Menghitung rasio vokal terhadap konsonan\"\"\"\n",
    "        vowel_count = sum(1 for char in text if char in vowels)\n",
    "        consonant_count = sum(1 for char in text if char in consonants)\n",
    "        \n",
    "        if consonant_count == 0:\n",
    "            return vowel_count if vowel_count > 0 else 0\n",
    "        return vowel_count / consonant_count\n",
    "    \n",
    "    # Fungsi untuk menghitung reduplikasi\n",
    "    def count_reduplication(text):\n",
    "        \"\"\"Menghitung jumlah kata yang mengandung reduplikasi\"\"\"\n",
    "        words = text.lower().split()\n",
    "        reduplication_count = 0\n",
    "        \n",
    "        for word in words:\n",
    "            # Cek reduplikasi penuh (kata-kata)\n",
    "            if '-' in word:\n",
    "                parts = word.split('-')\n",
    "                if len(parts) == 2 and parts[0] == parts[1]:\n",
    "                    reduplication_count += 1\n",
    "            \n",
    "            # Cek reduplikasi parsial (mengulang sebagian kata)\n",
    "            # Contoh: bebek, memem, gegap\n",
    "            for i in range(1, len(word)//2 + 1):\n",
    "                if word[:i] == word[i:2*i] and len(word[:i]) >= 2:\n",
    "                    reduplication_count += 1\n",
    "                    break\n",
    "                    \n",
    "        return reduplication_count\n",
    "    \n",
    "    # Fungsi untuk menghitung prefiks\n",
    "    def count_prefixes(text):\n",
    "        \"\"\"Menghitung jumlah kata yang mengandung prefiks\"\"\"\n",
    "        words = text.lower().split()\n",
    "        prefix_count = 0\n",
    "        \n",
    "        for word in words:\n",
    "            for prefix in common_prefixes:\n",
    "                if word.startswith(prefix) and len(word) > len(prefix):\n",
    "                    prefix_count += 1\n",
    "                    break\n",
    "                    \n",
    "        return prefix_count\n",
    "    \n",
    "    # Fungsi untuk menghitung sufiks\n",
    "    def count_suffixes(text):\n",
    "        \"\"\"Menghitung jumlah kata yang mengandung sufiks\"\"\"\n",
    "        words = text.lower().split()\n",
    "        suffix_count = 0\n",
    "        \n",
    "        for word in words:\n",
    "            for suffix in common_suffixes:\n",
    "                if word.endswith(suffix) and len(word) > len(suffix):\n",
    "                    suffix_count += 1\n",
    "                    break\n",
    "                    \n",
    "        return suffix_count\n",
    "    \n",
    "    # Ekstraksi n-gram untuk mendapatkan yang paling frequent\n",
    "    print(\"Menganalisis n-gram karakter...\")\n",
    "    char_2grams_all = []\n",
    "    char_3grams_all = []\n",
    "    \n",
    "    # Kumpulkan semua n-gram untuk mendapatkan yang paling frequent\n",
    "    for text in df_features['full_text']:\n",
    "        if pd.notna(text):\n",
    "            char_2grams_all.extend(get_char_ngrams(str(text), 2))\n",
    "            char_3grams_all.extend(get_char_ngrams(str(text), 3))\n",
    "    \n",
    "    # Dapatkan 10 teratas untuk 2-gram dan 3-gram\n",
    "    top_2grams = [item[0] for item in Counter(char_2grams_all).most_common(10)]\n",
    "    top_3grams = [item[0] for item in Counter(char_3grams_all).most_common(10)]\n",
    "    \n",
    "    print(f\"Top 10 2-grams: {top_2grams}\")\n",
    "    print(f\"Top 10 3-grams: {top_3grams}\")\n",
    "    \n",
    "    def extract_features_from_text(text):\n",
    "        \"\"\"Ekstraksi fitur dari satu teks\"\"\"\n",
    "        if pd.isna(text) or text.strip() == '':\n",
    "            return {feature: 0 for feature in fitur_penting_author_attribution}\n",
    "        \n",
    "        features = {}\n",
    "        text = str(text)\n",
    "        \n",
    "        # Preprocessing\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        # === FITUR STATISTIK TEKS ===\n",
    "        features['word_count'] = len(words)\n",
    "        \n",
    "        # Type-Token Ratio (TTR)\n",
    "        unique_words = len(set(words))\n",
    "        features['ttr'] = unique_words / len(words) if words else 0\n",
    "        \n",
    "        # Rata-rata panjang kata\n",
    "        features['avg_word_length'] = sum(len(word) for word in words) / len(words) if words else 0\n",
    "        \n",
    "        # Rata-rata suku kata per kata (estimasi sederhana)\n",
    "        def count_syllables(word):\n",
    "            vowels = 'aiueo'\n",
    "            count = sum(1 for char in word.lower() if char in vowels)\n",
    "            return max(1, count)\n",
    "        \n",
    "        features['avg_syllables_per_word'] = sum(count_syllables(word) for word in words) / len(words) if words else 0\n",
    "        \n",
    "        # Shannon Entropy\n",
    "        if words:\n",
    "            word_counts = Counter(words)\n",
    "            total = len(words)\n",
    "            entropy = -sum((count/total) * log2(count/total) for count in word_counts.values())\n",
    "            features['shannon_entropy'] = entropy\n",
    "        else:\n",
    "            features['shannon_entropy'] = 0\n",
    "        \n",
    "        # Yule's K\n",
    "        if words:\n",
    "            word_counts = Counter(words)\n",
    "            freq_freqs = Counter(word_counts.values())\n",
    "            N = len(words)\n",
    "            try:\n",
    "                yules_k = 10000 * (sum(freq * (freq_count**2) for freq, freq_count in freq_freqs.items()) - N) / (N**2)\n",
    "                features['yules_k'] = yules_k\n",
    "            except:\n",
    "                features['yules_k'] = 0\n",
    "        else:\n",
    "            features['yules_k'] = 0\n",
    "        \n",
    "        # === FITUR TANDA BACA ===\n",
    "        punct_counts = {\n",
    "            ',': text.count(','),\n",
    "            '.': text.count('.'),\n",
    "            '!': text.count('!'),\n",
    "            '?': text.count('?'),\n",
    "            '...': text.count('...'),\n",
    "            '\"': text.count('\"') + text.count(\"'\"),\n",
    "        }\n",
    "        \n",
    "        total_chars = len(text)\n",
    "        features['comma_ratio'] = punct_counts[','] / total_chars if total_chars > 0 else 0\n",
    "        features['period_ratio'] = punct_counts['.'] / total_chars if total_chars > 0 else 0\n",
    "        features['exclamation_mark_ratio'] = punct_counts['!'] / total_chars if total_chars > 0 else 0\n",
    "        features['question_mark_ratio'] = punct_counts['?'] / total_chars if total_chars > 0 else 0\n",
    "        features['ellipsis_count'] = punct_counts['...']\n",
    "        features['quotation_count'] = punct_counts['\"']\n",
    "        \n",
    "        # Total tanda baca\n",
    "        all_punct = re.findall(r'[^\\w\\s]', text)\n",
    "        features['total_punct_ratio'] = len(all_punct) / total_chars if total_chars > 0 else 0\n",
    "        \n",
    "        # Keragaman tanda baca\n",
    "        unique_punct = len(set(all_punct))\n",
    "        features['punct_diversity'] = unique_punct / len(all_punct) if all_punct else 0\n",
    "        \n",
    "        # === FITUR JENIS KATA ===\n",
    "        function_word_count = sum(1 for word in words if word in function_words)\n",
    "        content_word_count = len(words) - function_word_count\n",
    "        \n",
    "        features['function_ratio'] = function_word_count / len(words) if words else 0\n",
    "        features['content_ratio'] = content_word_count / len(words) if words else 0\n",
    "        features['function_to_content_ratio'] = function_word_count / content_word_count if content_word_count > 0 else 0\n",
    "        \n",
    "        # Kata ganti\n",
    "        pronoun_count = sum(1 for word in words if word in pronouns)\n",
    "        features['pronouns_ratio'] = pronoun_count / len(words) if words else 0\n",
    "        \n",
    "        # Kata sambung\n",
    "        conjunction_count = sum(1 for word in words if word in conjunctions)\n",
    "        features['conjunctions_ratio'] = conjunction_count / len(words) if words else 0\n",
    "        \n",
    "        # Kata depan\n",
    "        preposition_count = sum(1 for word in words if word in prepositions)\n",
    "        features['prepositions_ratio'] = preposition_count / len(words) if words else 0\n",
    "        \n",
    "        # === FITUR MARKER LINGUISTIK ===\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        features['marker_jadi'] = text_lower.count('jadi')\n",
    "        features['marker_menurut_saya'] = text_lower.count('menurut saya') + text_lower.count('menurut aku')\n",
    "        features['marker_bagi_saya'] = text_lower.count('bagi saya') + text_lower.count('bagi aku')\n",
    "        features['marker_faktanya'] = text_lower.count('faktanya') + text_lower.count('kenyataannya')\n",
    "        \n",
    "        total_markers = (features['marker_jadi'] + features['marker_menurut_saya'] + \n",
    "                        features['marker_bagi_saya'] + features['marker_faktanya'])\n",
    "        features['total_markers_count'] = total_markers\n",
    "        features['markers_ratio'] = total_markers / len(words) if words else 0\n",
    "        \n",
    "        # === FITUR MEDIA SOSIAL ===\n",
    "        features['hashtag_count'] = len(re.findall(r'#\\w+', text))\n",
    "        features['mention_count'] = len(re.findall(r'@\\w+', text))\n",
    "        features['url_count'] = len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n",
    "        \n",
    "        # === FITUR TAMBAHAN ===\n",
    "        features['double_dot_count'] = text.count('..')\n",
    "        \n",
    "        # Rata-rata frekuensi kata\n",
    "        if words:\n",
    "            word_frequencies = [word_freq.get(word, 0) / total_words for word in words]\n",
    "            features['avg_word_frequency'] = sum(word_frequencies) / len(word_frequencies)\n",
    "        else:\n",
    "            features['avg_word_frequency'] = 0\n",
    "        \n",
    "        # === FITUR TAMBAHAN BAHASA INDONESIA ===\n",
    "        # Character n-grams\n",
    "        text_2grams = get_char_ngrams(text, 2)\n",
    "        text_2gram_counter = Counter(text_2grams)\n",
    "        \n",
    "        for i, gram in enumerate(top_2grams):\n",
    "            features[f'char_2gram_freq_top10_{i+1}'] = text_2gram_counter.get(gram, 0)\n",
    "        \n",
    "        text_3grams = get_char_ngrams(text, 3)\n",
    "        text_3gram_counter = Counter(text_3grams)\n",
    "        \n",
    "        for i, gram in enumerate(top_3grams):\n",
    "            features[f'char_3gram_freq_top10_{i+1}'] = text_3gram_counter.get(gram, 0)\n",
    "        \n",
    "        # Fitur morfologi Indonesia\n",
    "        features['vowel_consonant_ratio'] = vowel_consonant_ratio(text)\n",
    "        features['reduplication_count'] = count_reduplication(text)\n",
    "        features['prefix_count'] = count_prefixes(text)\n",
    "        features['suffix_count'] = count_suffixes(text)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    # Ekstraksi fitur untuk setiap baris\n",
    "    print(\"Mengekstrak fitur author attribution...\")\n",
    "    \n",
    "    # Inisialisasi kolom fitur\n",
    "    for feature in fitur_penting_author_attribution:\n",
    "        df_features[feature] = 0.0\n",
    "    \n",
    "    # Inisialisasi kolom untuk n-gram features\n",
    "    for i in range(10):\n",
    "        df_features[f'char_2gram_freq_top10_{i+1}'] = 0\n",
    "        df_features[f'char_3gram_freq_top10_{i+1}'] = 0\n",
    "    \n",
    "    # Inisialisasi kolom untuk fitur bahasa Indonesia\n",
    "    df_features['vowel_consonant_ratio'] = 0.0\n",
    "    df_features['reduplication_count'] = 0\n",
    "    df_features['prefix_count'] = 0\n",
    "    df_features['suffix_count'] = 0\n",
    "    \n",
    "    # Ekstraksi fitur baris per baris\n",
    "    for idx, row in df_features.iterrows():\n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Memproses baris {idx+1}/{len(df_features)}\")\n",
    "        \n",
    "        text = row['full_text']\n",
    "        features = extract_features_from_text(text)\n",
    "        \n",
    "        # Assign fitur ke dataframe\n",
    "        for feature, value in features.items():\n",
    "            df_features.at[idx, feature] = value\n",
    "    \n",
    "    print(\"Ekstraksi fitur selesai!\")\n",
    "    return df_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be2be996-8489-44e9-937c-2dd1def4a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daftar fitur yang akan diekstrak\n",
    "fitur_penting_author_attribution = [\n",
    "    # === FITUR STATISTIK TEKS (Paling Kritis) ===\n",
    "    'word_count',                    # Panjang tweet - karakteristik kuat penulis\n",
    "    'ttr',                           # Type-Token Ratio - keragaman kosakata\n",
    "    'avg_word_length',              # Rata-rata panjang kata\n",
    "    'avg_syllables_per_word',       # Kompleksitas kata\n",
    "    'shannon_entropy',              # Entropi informasi - keragaman linguistik\n",
    "    'yules_k',                      # Ukuran keragaman leksikal yang robust\n",
    "    # === FITUR TANDA BACA (Sangat Kritis untuk Tweet) ===\n",
    "    'comma_ratio',                  # Penggunaan koma relatif\n",
    "    'period_ratio',                 # Penggunaan titik relatif\n",
    "    'exclamation_mark_ratio',       # Emosi/penekanan - khas untuk tweet\n",
    "    'question_mark_ratio',          # Gaya bertanya\n",
    "    'ellipsis_count',               # Titik tiga - gaya informal\n",
    "    'quotation_count',              # Penggunaan kutip\n",
    "    'total_punct_ratio',            # Total penggunaan tanda baca\n",
    "    'punct_diversity',              # Keragaman tanda baca\n",
    "    # === FITUR JENIS KATA (Penting untuk Gaya Bahasa) ===\n",
    "    'function_ratio',               # Rasio kata fungsi\n",
    "    'content_ratio',                # Rasio kata konten\n",
    "    'function_to_content_ratio',    # Perbandingan kata fungsi vs konten\n",
    "    'pronouns_ratio',               # Penggunaan kata ganti - sangat personal\n",
    "    'conjunctions_ratio',           # Gaya menghubungkan ide\n",
    "    'prepositions_ratio',           # Kompleksitas struktur kalimat\n",
    "    # === FITUR MARKER LINGUISTIK (Spesifik Bahasa Indonesia) ===\n",
    "    'marker_jadi',                  # Marker \"jadi\" - khas informal Indonesia\n",
    "    # 'marker_menurut_saya',          # Ekspresi opini personal\n",
    "    # 'marker_bagi_saya',             # Ekspresi perspektif personal\n",
    "    # 'marker_faktanya',              # Marker argumentasi\n",
    "    # 'total_markers_count',          # Total marker linguistik\n",
    "    # 'markers_ratio',                # Rasio penggunaan marker\n",
    "    # === FITUR MEDIA SOSIAL (Esensial untuk Tweet) ===\n",
    "    'hashtag_count',                # Penggunaan hashtag\n",
    "    'mention_count',                # Penggunaan mention (@)\n",
    "    'url_count',                    # Berbagi link\n",
    "    # === FITUR TAMBAHAN YANG RELEVAN ===\n",
    "    'double_dot_count',             # Penggunaan titik ganda (..)\n",
    "    'avg_word_frequency',           # Penggunaan kata umum vs jarang\n",
    "]\n",
    "\n",
    "# Fitur tambahan untuk bahasa Indonesia\n",
    "fitur_bahasa_indonesia = [\n",
    "    'char_2gram_freq_top10_1', 'char_2gram_freq_top10_2', 'char_2gram_freq_top10_3', 'char_2gram_freq_top10_4', 'char_2gram_freq_top10_5',\n",
    "    'char_2gram_freq_top10_6', 'char_2gram_freq_top10_7', 'char_2gram_freq_top10_8', 'char_2gram_freq_top10_9', 'char_2gram_freq_top10_10',\n",
    "    'char_3gram_freq_top10_1', 'char_3gram_freq_top10_2', 'char_3gram_freq_top10_3', 'char_3gram_freq_top10_4', 'char_3gram_freq_top10_5',\n",
    "    'char_3gram_freq_top10_6', 'char_3gram_freq_top10_7', 'char_3gram_freq_top10_8', 'char_3gram_freq_top10_9', 'char_3gram_freq_top10_10',\n",
    "    'vowel_consonant_ratio',\n",
    "    'reduplication_count',\n",
    "    'prefix_count',\n",
    "    'suffix_count'\n",
    "]\n",
    "\n",
    "# Gabungan semua fitur\n",
    "fitur_stilistik = fitur_penting_author_attribution + fitur_bahasa_indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4d65a74-b1a7-4188-a5c7-b7f44ece4a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menganalisis n-gram karakter...\n",
      "Top 10 2-grams: ['an', 'a ', 'n ', 'i ', 'ng', 'ka', ' p', 'er', 'ar', ' m']\n",
      "Top 10 3-grams: ['an ', 'ang', 'ng ', ' di', ' me', 'kan', 'ya ', 'nya', ' pe', 'ah ']\n",
      "Mengekstrak fitur author attribution...\n",
      "Memproses baris 1/14778\n",
      "Memproses baris 1001/14778\n",
      "Memproses baris 2001/14778\n",
      "Memproses baris 3001/14778\n",
      "Memproses baris 4001/14778\n",
      "Memproses baris 5001/14778\n",
      "Memproses baris 6001/14778\n",
      "Memproses baris 7001/14778\n",
      "Memproses baris 8001/14778\n",
      "Memproses baris 9001/14778\n",
      "Memproses baris 10001/14778\n",
      "Memproses baris 11001/14778\n",
      "Memproses baris 12001/14778\n",
      "Memproses baris 13001/14778\n",
      "Memproses baris 14001/14778\n",
      "Ekstraksi fitur selesai!\n",
      "Fitur statistik dasar:\n",
      "   word_count       ttr  avg_word_length  avg_syllables_per_word  \\\n",
      "0         2.0  0.500000        22.000000               10.000000   \n",
      "1         6.0  0.833333        10.500000                4.666667   \n",
      "2         3.0  0.666667        16.666667                7.666667   \n",
      "3         4.0  0.750000        13.750000                6.000000   \n",
      "4         7.0  0.857143        10.571429                4.714286   \n",
      "\n",
      "   shannon_entropy      yules_k  comma_ratio  period_ratio  \\\n",
      "0        -0.000000     0.000000          0.0      0.000000   \n",
      "1         2.251629  3333.333333          0.0      0.041096   \n",
      "2         0.918296     0.000000          0.0      0.000000   \n",
      "3         1.500000  1250.000000          0.0      0.016393   \n",
      "4         2.521641  4081.632653          0.0      0.012048   \n",
      "\n",
      "   exclamation_mark_ratio  question_mark_ratio  ...  marker_menurut_saya  \\\n",
      "0                     0.0                  0.0  ...                  0.0   \n",
      "1                     0.0                  0.0  ...                  0.0   \n",
      "2                     0.0                  0.0  ...                  0.0   \n",
      "3                     0.0                  0.0  ...                  0.0   \n",
      "4                     0.0                  0.0  ...                  0.0   \n",
      "\n",
      "   marker_bagi_saya  marker_faktanya  total_markers_count  markers_ratio  \\\n",
      "0               0.0              0.0                  0.0            0.0   \n",
      "1               0.0              0.0                  0.0            0.0   \n",
      "2               0.0              0.0                  0.0            0.0   \n",
      "3               0.0              0.0                  0.0            0.0   \n",
      "4               0.0              0.0                  0.0            0.0   \n",
      "\n",
      "   hashtag_count  mention_count  url_count  double_dot_count  \\\n",
      "0            2.0            0.0        0.0               0.0   \n",
      "1            2.0            0.0        0.0               1.0   \n",
      "2            2.0            0.0        0.0               0.0   \n",
      "3            2.0            0.0        0.0               0.0   \n",
      "4            2.0            0.0        0.0               0.0   \n",
      "\n",
      "   avg_word_frequency  \n",
      "0            0.000234  \n",
      "1            0.000725  \n",
      "2            0.000410  \n",
      "3            0.000544  \n",
      "4            0.001410  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "\n",
      "Fitur bahasa Indonesia:\n",
      "   char_2gram_freq_top10_1  char_2gram_freq_top10_2  char_2gram_freq_top10_3  \\\n",
      "0                        2                        0                        0   \n",
      "1                        2                        1                        0   \n",
      "2                        2                        0                        0   \n",
      "3                        3                        0                        1   \n",
      "4                        2                        1                        0   \n",
      "\n",
      "   char_2gram_freq_top10_4  char_2gram_freq_top10_5  char_2gram_freq_top10_6  \\\n",
      "0                        1                        0                        2   \n",
      "1                        2                        0                        3   \n",
      "2                        1                        0                        2   \n",
      "3                        1                        0                        3   \n",
      "4                        2                        0                        2   \n",
      "\n",
      "   char_2gram_freq_top10_7  char_2gram_freq_top10_8  char_2gram_freq_top10_9  \\\n",
      "0                        0                        0                        0   \n",
      "1                        0                        0                        0   \n",
      "2                        0                        0                        0   \n",
      "3                        0                        0                        1   \n",
      "4                        0                        0                        0   \n",
      "\n",
      "   char_2gram_freq_top10_10  ...  char_3gram_freq_top10_5  \\\n",
      "0                         0  ...                        0   \n",
      "1                         0  ...                        0   \n",
      "2                         0  ...                        0   \n",
      "3                         0  ...                        0   \n",
      "4                         2  ...                        0   \n",
      "\n",
      "   char_3gram_freq_top10_6  char_3gram_freq_top10_7  char_3gram_freq_top10_8  \\\n",
      "0                        2                        0                        0   \n",
      "1                        2                        0                        0   \n",
      "2                        2                        0                        0   \n",
      "3                        3                        0                        0   \n",
      "4                        2                        1                        1   \n",
      "\n",
      "   char_3gram_freq_top10_9  char_3gram_freq_top10_10  vowel_consonant_ratio  \\\n",
      "0                        0                         0               0.833333   \n",
      "1                        0                         1               0.800000   \n",
      "2                        0                         0               0.851852   \n",
      "3                        0                         0               0.718750   \n",
      "4                        0                         0               0.804878   \n",
      "\n",
      "   reduplication_count  prefix_count  suffix_count  \n",
      "0                    0             0             2  \n",
      "1                    0             0             4  \n",
      "2                    0             1             2  \n",
      "3                    0             0             3  \n",
      "4                    0             0             4  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "Top 10 Character 2-grams yang dianalisis:\n",
      "Position 1: char_2gram_freq_top10_1\n",
      "Position 2: char_2gram_freq_top10_2\n",
      "Position 3: char_2gram_freq_top10_3\n",
      "Position 4: char_2gram_freq_top10_4\n",
      "Position 5: char_2gram_freq_top10_5\n",
      "Position 6: char_2gram_freq_top10_6\n",
      "Position 7: char_2gram_freq_top10_7\n",
      "Position 8: char_2gram_freq_top10_8\n",
      "Position 9: char_2gram_freq_top10_9\n",
      "Position 10: char_2gram_freq_top10_10\n",
      "\n",
      "Top 10 Character 3-grams yang dianalisis:\n",
      "Position 1: char_3gram_freq_top10_1\n",
      "Position 2: char_3gram_freq_top10_2\n",
      "Position 3: char_3gram_freq_top10_3\n",
      "Position 4: char_3gram_freq_top10_4\n",
      "Position 5: char_3gram_freq_top10_5\n",
      "Position 6: char_3gram_freq_top10_6\n",
      "Position 7: char_3gram_freq_top10_7\n",
      "Position 8: char_3gram_freq_top10_8\n",
      "Position 9: char_3gram_freq_top10_9\n",
      "Position 10: char_3gram_freq_top10_10\n"
     ]
    }
   ],
   "source": [
    "# Contoh penggunaan:\n",
    "\n",
    "# Assuming you have a dataframe 'df' with 'full_text' and 'username' columns\n",
    "df_with_features = extract_author_attribution_features(df)\n",
    "\n",
    "# Melihat semua fitur yang telah diekstrak\n",
    "print(\"Fitur statistik dasar:\")\n",
    "print(df_with_features[fitur_penting_author_attribution].head())\n",
    "\n",
    "print(\"\\nFitur bahasa Indonesia:\")\n",
    "print(df_with_features[fitur_bahasa_indonesia].head())\n",
    "\n",
    "# Melihat top n-grams yang digunakan\n",
    "print(\"\\nTop 10 Character 2-grams yang dianalisis:\")\n",
    "for i in range(10):\n",
    "    col_name = f'char_2gram_freq_top10_{i+1}'\n",
    "    if col_name in df_with_features.columns:\n",
    "        print(f\"Position {i+1}: {col_name}\")\n",
    "\n",
    "print(\"\\nTop 10 Character 3-grams yang dianalisis:\")\n",
    "for i in range(10):\n",
    "    col_name = f'char_3gram_freq_top10_{i+1}'\n",
    "    if col_name in df_with_features.columns:\n",
    "        print(f\"Position {i+1}: {col_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3db8e7c-5974-4f15-bbeb-cc53e0a9ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_features.to_csv(\"Dangerous_full_feature_raw.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28a45799-05ed-4940-8f38-e055f2152f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocessing teks untuk tweet bahasa Indonesia\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def extract_tfidf_pca_features(df, text_column='full_text', n_components=80):\n",
    "    \"\"\"\n",
    "    Ekstraksi fitur TF-IDF dan PCA untuk author attribution\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame yang berisi data tweet\n",
    "    text_column : str\n",
    "        Nama kolom yang berisi teks tweet (default: 'full_text')\n",
    "    n_components : int\n",
    "        Jumlah komponen PCA (default: 80)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame asli dengan tambahan kolom fitur tfidf_1, tfidf_2, ..., tfidf_n\n",
    "    \"\"\"\n",
    "    \n",
    "    # Copy dataframe untuk menghindari perubahan pada data asli\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Preprocessing teks\n",
    "    df_copy['processed_text'] = df_copy[text_column].apply(preprocess_text)\n",
    "    \n",
    "    # Hapus baris dengan teks kosong\n",
    "    df_copy = df_copy[df_copy['processed_text'].str.len() > 0].reset_index(drop=True)\n",
    "    \n",
    "    # Ekstraksi fitur TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=None,\n",
    "        ngram_range=(1, 3),\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        stop_words=None,\n",
    "        lowercase=True,\n",
    "        sublinear_tf=True,\n",
    "        norm='l2'\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df_copy['processed_text'])\n",
    "    \n",
    "    # PCA untuk reduksi dimensi\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    tfidf_pca = pca.fit_transform(tfidf_matrix.toarray())\n",
    "    \n",
    "    # Buat nama kolom untuk fitur PCA\n",
    "    pca_columns = [f'tfidf_{i+1}' for i in range(n_components)]\n",
    "    \n",
    "    # Tambahkan fitur ke DataFrame\n",
    "    for i, col in enumerate(pca_columns):\n",
    "        df_copy[col] = tfidf_pca[:, i]\n",
    "    \n",
    "    # Hapus kolom processed_text (tidak diperlukan)\n",
    "    df_copy = df_copy.drop('processed_text', axis=1)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0484f5fe-d5e4-4f2a-bf1c-af46b2339fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_features = extract_tfidf_pca_features(df_with_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42a14803-37ed-49db-b65b-beacd715c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_features.to_csv(\"Dangerous_full_feature_raw_tfidf.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c8d052a-84fd-4636-a418-c4a241667b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_olah = pd.read_csv(\"Dangerous_full_feature_raw_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efe20e71-a109-4e3e-abb1-499b086271c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai Analisis Clustering untuk Author Attribution\n",
      "============================================================\n",
      "Shape dataset: (14778, 233)\n",
      "Kolom tersedia: ['conversation_id_str', 'created_at', 'favorite_count', 'full_text', 'id_str', 'image_url', 'in_reply_to_screen_name', 'lang', 'location', 'quote_count', 'reply_count', 'retweet_count', 'tweet_url', 'user_id_str', 'username', 'full_text_clean', 'full_text_clean_2', 'indobert_feature_1', 'indobert_feature_2', 'indobert_feature_3', 'indobert_feature_4', 'indobert_feature_5', 'indobert_feature_6', 'indobert_feature_7', 'indobert_feature_8', 'indobert_feature_9', 'indobert_feature_10', 'indobert_feature_11', 'indobert_feature_12', 'indobert_feature_13', 'indobert_feature_14', 'indobert_feature_15', 'indobert_feature_16', 'indobert_feature_17', 'indobert_feature_18', 'indobert_feature_19', 'indobert_feature_20', 'indobert_feature_21', 'indobert_feature_22', 'indobert_feature_23', 'indobert_feature_24', 'indobert_feature_25', 'indobert_feature_26', 'indobert_feature_27', 'indobert_feature_28', 'indobert_feature_29', 'indobert_feature_30', 'indobert_feature_31', 'indobert_feature_32', 'indobert_feature_33', 'indobert_feature_34', 'indobert_feature_35', 'indobert_feature_36', 'indobert_feature_37', 'indobert_feature_38', 'indobert_feature_39', 'indobert_feature_40', 'indobert_feature_41', 'indobert_feature_42', 'indobert_feature_43', 'indobert_feature_44', 'indobert_feature_45', 'indobert_feature_46', 'indobert_feature_47', 'indobert_feature_48', 'indobert_feature_49', 'indobert_feature_50', 'indobert_feature_51', 'indobert_feature_52', 'indobert_feature_53', 'indobert_feature_54', 'indobert_feature_55', 'indobert_feature_56', 'indobert_feature_57', 'indobert_feature_58', 'indobert_feature_59', 'indobert_feature_60', 'indobert_feature_61', 'indobert_feature_62', 'indobert_feature_63', 'indobert_feature_64', 'indobert_feature_65', 'indobert_feature_66', 'indobert_feature_67', 'indobert_feature_68', 'indobert_feature_69', 'indobert_feature_70', 'indobert_feature_71', 'indobert_feature_72', 'indobert_feature_73', 'indobert_feature_74', 'indobert_feature_75', 'indobert_feature_76', 'indobert_feature_77', 'indobert_feature_78', 'indobert_feature_79', 'indobert_feature_80', 'word_count', 'ttr', 'avg_word_length', 'avg_syllables_per_word', 'shannon_entropy', 'yules_k', 'comma_ratio', 'period_ratio', 'exclamation_mark_ratio', 'question_mark_ratio', 'ellipsis_count', 'quotation_count', 'total_punct_ratio', 'punct_diversity', 'function_ratio', 'content_ratio', 'function_to_content_ratio', 'pronouns_ratio', 'conjunctions_ratio', 'prepositions_ratio', 'marker_jadi', 'marker_menurut_saya', 'marker_bagi_saya', 'marker_faktanya', 'total_markers_count', 'markers_ratio', 'hashtag_count', 'mention_count', 'url_count', 'double_dot_count', 'avg_word_frequency', 'char_2gram_freq_top10_1', 'char_3gram_freq_top10_1', 'char_2gram_freq_top10_2', 'char_3gram_freq_top10_2', 'char_2gram_freq_top10_3', 'char_3gram_freq_top10_3', 'char_2gram_freq_top10_4', 'char_3gram_freq_top10_4', 'char_2gram_freq_top10_5', 'char_3gram_freq_top10_5', 'char_2gram_freq_top10_6', 'char_3gram_freq_top10_6', 'char_2gram_freq_top10_7', 'char_3gram_freq_top10_7', 'char_2gram_freq_top10_8', 'char_3gram_freq_top10_8', 'char_2gram_freq_top10_9', 'char_3gram_freq_top10_9', 'char_2gram_freq_top10_10', 'char_3gram_freq_top10_10', 'vowel_consonant_ratio', 'reduplication_count', 'prefix_count', 'suffix_count', 'tfidf_1', 'tfidf_2', 'tfidf_3', 'tfidf_4', 'tfidf_5', 'tfidf_6', 'tfidf_7', 'tfidf_8', 'tfidf_9', 'tfidf_10', 'tfidf_11', 'tfidf_12', 'tfidf_13', 'tfidf_14', 'tfidf_15', 'tfidf_16', 'tfidf_17', 'tfidf_18', 'tfidf_19', 'tfidf_20', 'tfidf_21', 'tfidf_22', 'tfidf_23', 'tfidf_24', 'tfidf_25', 'tfidf_26', 'tfidf_27', 'tfidf_28', 'tfidf_29', 'tfidf_30', 'tfidf_31', 'tfidf_32', 'tfidf_33', 'tfidf_34', 'tfidf_35', 'tfidf_36', 'tfidf_37', 'tfidf_38', 'tfidf_39', 'tfidf_40', 'tfidf_41', 'tfidf_42', 'tfidf_43', 'tfidf_44', 'tfidf_45', 'tfidf_46', 'tfidf_47', 'tfidf_48', 'tfidf_49', 'tfidf_50', 'tfidf_51', 'tfidf_52', 'tfidf_53', 'tfidf_54', 'tfidf_55', 'tfidf_56', 'tfidf_57', 'tfidf_58', 'tfidf_59', 'tfidf_60', 'tfidf_61', 'tfidf_62', 'tfidf_63', 'tfidf_64', 'tfidf_65', 'tfidf_66', 'tfidf_67', 'tfidf_68', 'tfidf_69', 'tfidf_70', 'tfidf_71', 'tfidf_72', 'tfidf_73', 'tfidf_74', 'tfidf_75', 'tfidf_76', 'tfidf_77', 'tfidf_78', 'tfidf_79', 'tfidf_80', 'dummy_label']\n",
      "Jumlah cluster sebenarnya: 11\n",
      "Fitur Stilistik ditemukan: 50 fitur\n",
      "TF-IDF ditemukan: 80 fitur\n",
      "IndoBERT ditemukan: 80 fitur\n",
      "\n",
      "Memulai eksperimen clustering...\n",
      "============================================================\n",
      "\n",
      "Kombinasi Fitur: A_only (50 fitur)\n",
      "  Testing KMeans...\n",
      "    Clusters: 11, NMI: 0.0943, ARI: 0.0698, Time: 0.1177s\n",
      "  Testing Gaussian_Mixture...\n",
      "    Clusters: 11, NMI: 0.2074, ARI: 0.1555, Time: 4.1493s\n",
      "  Testing Agglomerative_Ward...\n",
      "    Clusters: 11, NMI: 0.0956, ARI: 0.0683, Time: 4.5892s\n",
      "  Testing Agglomerative_Complete...\n",
      "    Clusters: 11, NMI: 0.0976, ARI: 0.0833, Time: 4.4849s\n",
      "  Testing DBSCAN_auto...\n",
      "    Clusters: 29, NMI: 0.0212, ARI: 0.0002, Time: 0.0825s\n",
      "  Testing DBSCAN_tight...\n",
      "    Clusters: 73, NMI: 0.0368, ARI: 0.0004, Time: 0.0820s\n",
      "\n",
      "Kombinasi Fitur: B_only (80 fitur)\n",
      "  Testing KMeans...\n",
      "    Clusters: 11, NMI: 0.2094, ARI: 0.0680, Time: 0.4961s\n",
      "  Testing Gaussian_Mixture...\n",
      "    Clusters: 11, NMI: 0.3122, ARI: 0.2233, Time: 8.7647s\n",
      "  Testing Agglomerative_Ward...\n",
      "    Clusters: 11, NMI: 0.1850, ARI: 0.0182, Time: 5.7522s\n",
      "  Testing Agglomerative_Complete...\n",
      "    Clusters: 11, NMI: 0.0391, ARI: 0.0003, Time: 5.5880s\n",
      "  Testing DBSCAN_auto...\n",
      "    Clusters: 2, NMI: 0.0003, ARI: 0.0000, Time: 1.2508s\n",
      "  Testing DBSCAN_tight...\n",
      "    Clusters: 24, NMI: 0.0399, ARI: 0.0006, Time: 0.6653s\n",
      "\n",
      "Kombinasi Fitur: C_only (80 fitur)\n",
      "  Testing KMeans...\n",
      "    Clusters: 11, NMI: 0.3894, ARI: 0.2320, Time: 0.9885s\n",
      "  Testing Gaussian_Mixture...\n",
      "    Clusters: 11, NMI: 0.4363, ARI: 0.3073, Time: 4.9721s\n",
      "  Testing Agglomerative_Ward...\n",
      "    Clusters: 11, NMI: 0.4037, ARI: 0.2514, Time: 5.7157s\n",
      "  Testing Agglomerative_Complete...\n",
      "    Clusters: 11, NMI: 0.1991, ARI: 0.0725, Time: 5.5754s\n",
      "  Testing DBSCAN_auto...\n",
      "    Clusters: 19, NMI: 0.0157, ARI: -0.0001, Time: 0.1164s\n",
      "  Testing DBSCAN_tight...\n",
      "    Clusters: 49, NMI: 0.0274, ARI: -0.0000, Time: 0.1116s\n",
      "\n",
      "Kombinasi Fitur: A+B (130 fitur)\n",
      "  Testing KMeans...\n",
      "    Clusters: 11, NMI: 0.0943, ARI: 0.0698, Time: 0.3430s\n",
      "  Testing Gaussian_Mixture...\n",
      "    Clusters: 11, NMI: 0.3039, ARI: 0.2193, Time: 14.8474s\n",
      "  Testing Agglomerative_Ward...\n",
      "    Clusters: 11, NMI: 0.0956, ARI: 0.0683, Time: 6.5929s\n",
      "  Testing Agglomerative_Complete...\n",
      "    Clusters: 11, NMI: 0.0976, ARI: 0.0833, Time: 6.4898s\n",
      "  Testing DBSCAN_auto...\n",
      "    Clusters: 26, NMI: 0.0196, ARI: 0.0001, Time: 0.1682s\n",
      "  Testing DBSCAN_tight...\n",
      "    Clusters: 74, NMI: 0.0362, ARI: 0.0003, Time: 0.1689s\n",
      "\n",
      "Kombinasi Fitur: A+C (130 fitur)\n",
      "  Testing KMeans...\n",
      "    Clusters: 11, NMI: 0.0943, ARI: 0.0698, Time: 0.3252s\n",
      "  Testing Gaussian_Mixture...\n",
      "    Clusters: 11, NMI: 0.2893, ARI: 0.2286, Time: 14.7074s\n",
      "  Testing Agglomerative_Ward...\n",
      "    Clusters: 11, NMI: 0.0962, ARI: 0.0762, Time: 6.4805s\n",
      "  Testing Agglomerative_Complete...\n",
      "    Clusters: 11, NMI: 0.0998, ARI: 0.0811, Time: 6.2927s\n",
      "  Testing DBSCAN_auto...\n",
      "    Clusters: 19, NMI: 0.0157, ARI: -0.0001, Time: 0.1654s\n",
      "  Testing DBSCAN_tight...\n",
      "    Clusters: 48, NMI: 0.0271, ARI: -0.0000, Time: 0.1659s\n",
      "\n",
      "Kombinasi Fitur: B+C (160 fitur)\n",
      "  Testing KMeans...\n",
      "    Clusters: 11, NMI: 0.3893, ARI: 0.2320, Time: 1.3825s\n",
      "  Testing Gaussian_Mixture...\n",
      "    Clusters: 11, NMI: 0.3826, ARI: 0.2565, Time: 21.3627s\n",
      "  Testing Agglomerative_Ward...\n",
      "    Clusters: 11, NMI: 0.3898, ARI: 0.2311, Time: 7.7994s\n",
      "  Testing Agglomerative_Complete...\n",
      "    Clusters: 11, NMI: 0.2213, ARI: 0.1285, Time: 7.6215s\n",
      "  Testing DBSCAN_auto...\n",
      "    Clusters: 19, NMI: 0.0157, ARI: -0.0001, Time: 0.2027s\n",
      "  Testing DBSCAN_tight...\n",
      "    Clusters: 48, NMI: 0.0271, ARI: -0.0000, Time: 0.1993s\n",
      "\n",
      "Kombinasi Fitur: A+B+C (210 fitur)\n",
      "  Testing KMeans...\n",
      "    Clusters: 11, NMI: 0.0943, ARI: 0.0698, Time: 0.5592s\n",
      "  Testing Gaussian_Mixture...\n",
      "    Clusters: 11, NMI: 0.2501, ARI: 0.1736, Time: 26.6731s\n",
      "  Testing Agglomerative_Ward...\n",
      "    Clusters: 11, NMI: 0.0962, ARI: 0.0762, Time: 9.0482s\n",
      "  Testing Agglomerative_Complete...\n",
      "    Clusters: 11, NMI: 0.0998, ARI: 0.0811, Time: 8.8437s\n",
      "  Testing DBSCAN_auto...\n",
      "    Clusters: 19, NMI: 0.0157, ARI: -0.0001, Time: 0.2567s\n",
      "  Testing DBSCAN_tight...\n",
      "    Clusters: 48, NMI: 0.0271, ARI: -0.0000, Time: 0.2578s\n",
      "\n",
      "Hasil disimpan ke: author_attribution_result_raw.csv\n",
      "\n",
      "================================================================================\n",
      "HASIL EKSPERIMEN CLUSTERING\n",
      "================================================================================\n",
      "Total eksperimen: 42\n",
      "Kombinasi fitur: 7\n",
      "Algoritma: 6\n",
      "\n",
      "TOP 10 BERDASARKAN NMI:\n",
      "--------------------------------------------------\n",
      "Feature_Combination          Algorithm      NMI      ARI  Runtime_seconds\n",
      "             C_only   Gaussian_Mixture 0.436251 0.307345         4.972110\n",
      "             C_only Agglomerative_Ward 0.403742 0.251415         5.715722\n",
      "                B+C Agglomerative_Ward 0.389843 0.231056         7.799373\n",
      "             C_only             KMeans 0.389432 0.231951         0.988530\n",
      "                B+C             KMeans 0.389250 0.232006         1.382492\n",
      "                B+C   Gaussian_Mixture 0.382601 0.256454        21.362733\n",
      "             B_only   Gaussian_Mixture 0.312204 0.223278         8.764673\n",
      "                A+B   Gaussian_Mixture 0.303856 0.219299        14.847405\n",
      "                A+C   Gaussian_Mixture 0.289280 0.228598        14.707381\n",
      "              A+B+C   Gaussian_Mixture 0.250100 0.173645        26.673140\n",
      "\n",
      "TOP 10 BERDASARKAN ARI:\n",
      "--------------------------------------------------\n",
      "Feature_Combination          Algorithm      NMI      ARI  Runtime_seconds\n",
      "             C_only   Gaussian_Mixture 0.436251 0.307345         4.972110\n",
      "                B+C   Gaussian_Mixture 0.382601 0.256454        21.362733\n",
      "             C_only Agglomerative_Ward 0.403742 0.251415         5.715722\n",
      "                B+C             KMeans 0.389250 0.232006         1.382492\n",
      "             C_only             KMeans 0.389432 0.231951         0.988530\n",
      "                B+C Agglomerative_Ward 0.389843 0.231056         7.799373\n",
      "                A+C   Gaussian_Mixture 0.289280 0.228598        14.707381\n",
      "             B_only   Gaussian_Mixture 0.312204 0.223278         8.764673\n",
      "                A+B   Gaussian_Mixture 0.303856 0.219299        14.847405\n",
      "              A+B+C   Gaussian_Mixture 0.250100 0.173645        26.673140\n",
      "\n",
      "RINGKASAN PER KOMBINASI FITUR:\n",
      "--------------------------------------------------\n",
      "                        NMI                     ARI                  \\\n",
      "                       mean     max     std    mean     max     std   \n",
      "Feature_Combination                                                   \n",
      "A+B                  0.1079  0.3039  0.1018  0.0735  0.2193  0.0802   \n",
      "A+B+C                0.0972  0.2501  0.0836  0.0668  0.1736  0.0643   \n",
      "A+C                  0.1037  0.2893  0.0982  0.0759  0.2286  0.0836   \n",
      "A_only               0.0922  0.2074  0.0655  0.0629  0.1555  0.0581   \n",
      "B+C                  0.2376  0.3898  0.1795  0.1413  0.2565  0.1180   \n",
      "B_only               0.1310  0.3122  0.1231  0.0517  0.2233  0.0880   \n",
      "C_only               0.2453  0.4363  0.1922  0.1438  0.3073  0.1361   \n",
      "\n",
      "                    Runtime_seconds                   \n",
      "                               mean     min      max  \n",
      "Feature_Combination                                   \n",
      "A+B                          4.7684  0.1682  14.8474  \n",
      "A+B+C                        7.6064  0.2567  26.6731  \n",
      "A+C                          4.6895  0.1654  14.7074  \n",
      "A_only                       2.2509  0.0820   4.5892  \n",
      "B+C                          6.4280  0.1993  21.3627  \n",
      "B_only                       3.7528  0.4961   8.7647  \n",
      "C_only                       2.9133  0.1116   5.7157  \n",
      "\n",
      "RINGKASAN PER ALGORITMA:\n",
      "--------------------------------------------------\n",
      "                           NMI                     ARI                  \\\n",
      "                          mean     max     std    mean     max     std   \n",
      "Algorithm                                                                \n",
      "Agglomerative_Complete  0.1220  0.2213  0.0644  0.0757  0.1285  0.0380   \n",
      "Agglomerative_Ward      0.1946  0.4037  0.1420  0.1128  0.2514  0.0902   \n",
      "DBSCAN_auto             0.0148  0.0212  0.0068 -0.0000  0.0002  0.0001   \n",
      "DBSCAN_tight            0.0317  0.0399  0.0057  0.0002  0.0006  0.0003   \n",
      "Gaussian_Mixture        0.3117  0.4363  0.0772  0.2234  0.3073  0.0505   \n",
      "KMeans                  0.1951  0.3894  0.1392  0.1159  0.2320  0.0793   \n",
      "\n",
      "                       Runtime_seconds                   \n",
      "                                  mean     min      max  \n",
      "Algorithm                                                \n",
      "Agglomerative_Complete          6.4137  4.4849   8.8437  \n",
      "Agglomerative_Ward              6.5683  4.5892   9.0482  \n",
      "DBSCAN_auto                     0.3204  0.0825   1.2508  \n",
      "DBSCAN_tight                    0.2358  0.0820   0.6653  \n",
      "Gaussian_Mixture               13.6395  4.1493  26.6731  \n",
      "KMeans                          0.6017  0.1177   1.3825  \n",
      "Script siap digunakan!\n",
      "Untuk menjalankan eksperimen, gunakan:\n",
      "results = run_clustering_experiment(df, 'nama_kolom_label', 'output_file_.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def prepare_feature_combinations(df):\n",
    "    \"\"\"\n",
    "    Menyiapkan berbagai kombinasi fitur\n",
    "    \"\"\"\n",
    "    # Identifikasi kolom fitur\n",
    "    tfidf_features = [col for col in df.columns if col.startswith('tfidf_')]\n",
    "    indobert_features = [col for col in df.columns if col.startswith('indobert_feature_')]\n",
    "    \n",
    "    # Jika tidak ditemukan dengan nama persis, coba alternatif\n",
    "    \n",
    "    if not tfidf_features:\n",
    "        tfidf_features = [f'tfidf_{i}' for i in range(1, 81) if f'tfidf_{i}' in df.columns]\n",
    "    \n",
    "    if not indobert_features:\n",
    "        indobert_features = [f'indobert_feature_{i}' for i in range(1, 81) if f'indobert_feature_{i}' in df.columns]\n",
    "    \n",
    "    print(f\"Fitur Stilistik ditemukan: {len(fitur_stilistik)} fitur\")\n",
    "    print(f\"TF-IDF ditemukan: {len(tfidf_features)} fitur\")\n",
    "    print(f\"IndoBERT ditemukan: {len(indobert_features)} fitur\")\n",
    "    \n",
    "    # Kombinasi fitur\n",
    "    combinations = {\n",
    "        'A_only': fitur_stilistik,\n",
    "        'B_only': tfidf_features,\n",
    "        'C_only': indobert_features,\n",
    "        'A+B': fitur_stilistik + tfidf_features,\n",
    "        'A+C': fitur_stilistik + indobert_features,\n",
    "        'B+C': tfidf_features + indobert_features,\n",
    "        'A+B+C': fitur_stilistik + tfidf_features + indobert_features\n",
    "    }\n",
    "    \n",
    "    return combinations\n",
    "\n",
    "def get_clustering_algorithms(n_clusters):\n",
    "    \"\"\"\n",
    "    Algoritma clustering yang direkomendasikan untuk author attribution\n",
    "    \"\"\"\n",
    "    algorithms = {\n",
    "        'KMeans': KMeans(n_clusters=n_clusters, random_state=42, n_init=10),\n",
    "        'Gaussian_Mixture': GaussianMixture(n_components=n_clusters, random_state=42),\n",
    "        'Agglomerative_Ward': AgglomerativeClustering(n_clusters=n_clusters, linkage='ward'),\n",
    "        'Agglomerative_Complete': AgglomerativeClustering(n_clusters=n_clusters, linkage='complete'),\n",
    "        'DBSCAN_auto': DBSCAN(eps=0.5, min_samples=5),\n",
    "        'DBSCAN_tight': DBSCAN(eps=0.3, min_samples=3)\n",
    "    }\n",
    "    return algorithms\n",
    "\n",
    "def perform_clustering_experiment(df, true_labels_col='label'):\n",
    "    \"\"\"\n",
    "    Melakukan eksperimen clustering dengan berbagai kombinasi fitur dan algoritma\n",
    "    \"\"\"\n",
    "    # Persiapan\n",
    "    if true_labels_col not in df.columns:\n",
    "        print(f\"Kolom {true_labels_col} tidak ditemukan. Mencari kolom label...\")\n",
    "        label_cols = [col for col in df.columns if 'label' in col.lower() or 'author' in col.lower()]\n",
    "        if label_cols:\n",
    "            true_labels_col = label_cols[0]\n",
    "            print(f\"Menggunakan kolom: {true_labels_col}\")\n",
    "        else:\n",
    "            print(\"Tidak ada kolom label ditemukan. Menggunakan dummy labels.\")\n",
    "            df['dummy_label'] = np.random.randint(0, 5, len(df))\n",
    "            true_labels_col = 'dummy_label'\n",
    "    \n",
    "    true_labels = df[true_labels_col].values\n",
    "    n_true_clusters = len(np.unique(true_labels))\n",
    "    print(f\"Jumlah cluster sebenarnya: {n_true_clusters}\")\n",
    "    \n",
    "    # Siapkan kombinasi fitur\n",
    "    feature_combinations = prepare_feature_combinations(df)\n",
    "    \n",
    "    # Siapkan algoritma\n",
    "    algorithms = get_clustering_algorithms(n_true_clusters)\n",
    "    \n",
    "    # Hasil eksperimen\n",
    "    results = []\n",
    "    \n",
    "    # Scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    print(\"\\nMemulai eksperimen clustering...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Loop melalui setiap kombinasi fitur\n",
    "    for combo_name, features in feature_combinations.items():\n",
    "        if not features:\n",
    "            print(f\"Kombinasi {combo_name} kosong, dilewati.\")\n",
    "            continue\n",
    "            \n",
    "        # Cek apakah semua fitur ada di dataframe\n",
    "        available_features = [f for f in features if f in df.columns]\n",
    "        if not available_features:\n",
    "            print(f\"Tidak ada fitur tersedia untuk kombinasi {combo_name}, dilewati.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nKombinasi Fitur: {combo_name} ({len(available_features)} fitur)\")\n",
    "        \n",
    "        # Siapkan data\n",
    "        X = df[available_features].values\n",
    "        \n",
    "        # Handle missing values\n",
    "        if np.isnan(X).any():\n",
    "            X = np.nan_to_num(X, nan=0.0)\n",
    "        \n",
    "        # Normalisasi\n",
    "        X_scaled = X\n",
    "        \n",
    "        # Loop melalui setiap algoritma\n",
    "        for algo_name, algorithm in algorithms.items():\n",
    "            print(f\"  Testing {algo_name}...\")\n",
    "            \n",
    "            try:\n",
    "                # Ukur waktu\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Clustering\n",
    "                if hasattr(algorithm, 'fit_predict'):\n",
    "                    cluster_labels = algorithm.fit_predict(X_scaled)\n",
    "                else:\n",
    "                    algorithm.fit(X_scaled)\n",
    "                    cluster_labels = algorithm.predict(X_scaled)\n",
    "                \n",
    "                end_time = time.time()\n",
    "                runtime = end_time - start_time\n",
    "                \n",
    "                # Evaluasi - hanya jika ada lebih dari 1 cluster\n",
    "                n_clusters_found = len(np.unique(cluster_labels))\n",
    "                \n",
    "                if n_clusters_found > 1:\n",
    "                    nmi = normalized_mutual_info_score(true_labels, cluster_labels)\n",
    "                    ari = adjusted_rand_score(true_labels, cluster_labels)\n",
    "                else:\n",
    "                    nmi = 0.0\n",
    "                    ari = 0.0\n",
    "                \n",
    "                # Simpan hasil\n",
    "                results.append({\n",
    "                    'Feature_Combination': combo_name,\n",
    "                    'Algorithm': algo_name,\n",
    "                    'N_Features': len(available_features),\n",
    "                    'N_Clusters_Found': n_clusters_found,\n",
    "                    'NMI': nmi,\n",
    "                    'ARI': ari,\n",
    "                    'Runtime_seconds': runtime\n",
    "                })\n",
    "                \n",
    "                print(f\"    Clusters: {n_clusters_found}, NMI: {nmi:.4f}, ARI: {ari:.4f}, Time: {runtime:.4f}s\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error dengan {algo_name}: {str(e)}\")\n",
    "                # Simpan hasil error\n",
    "                results.append({\n",
    "                    'Feature_Combination': combo_name,\n",
    "                    'Algorithm': algo_name,\n",
    "                    'N_Features': len(available_features),\n",
    "                    'N_Clusters_Found': 0,\n",
    "                    'NMI': 0.0,\n",
    "                    'ARI': 0.0,\n",
    "                    'Runtime_seconds': 0.0\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def analyze_and_save_results(results_df, filename='clustering_results.csv'):\n",
    "    \"\"\"\n",
    "    Analisis dan simpan hasil\n",
    "    \"\"\"\n",
    "    # Simpan ke CSV\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    print(f\"\\nHasil disimpan ke: {filename}\")\n",
    "    \n",
    "    # Print hasil\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HASIL EKSPERIMEN CLUSTERING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Statistik umum\n",
    "    print(f\"Total eksperimen: {len(results_df)}\")\n",
    "    print(f\"Kombinasi fitur: {results_df['Feature_Combination'].nunique()}\")\n",
    "    print(f\"Algoritma: {results_df['Algorithm'].nunique()}\")\n",
    "    \n",
    "    # Top 10 berdasarkan NMI\n",
    "    print(\"\\nTOP 10 BERDASARKAN NMI:\")\n",
    "    print(\"-\" * 50)\n",
    "    top_nmi = results_df.nlargest(10, 'NMI')[['Feature_Combination', 'Algorithm', 'NMI', 'ARI', 'Runtime_seconds']]\n",
    "    print(top_nmi.to_string(index=False))\n",
    "    \n",
    "    # Top 10 berdasarkan ARI\n",
    "    print(\"\\nTOP 10 BERDASARKAN ARI:\")\n",
    "    print(\"-\" * 50)\n",
    "    top_ari = results_df.nlargest(10, 'ARI')[['Feature_Combination', 'Algorithm', 'NMI', 'ARI', 'Runtime_seconds']]\n",
    "    print(top_ari.to_string(index=False))\n",
    "    \n",
    "    # Ringkasan per kombinasi fitur\n",
    "    print(\"\\nRINGKASAN PER KOMBINASI FITUR:\")\n",
    "    print(\"-\" * 50)\n",
    "    feature_summary = results_df.groupby('Feature_Combination').agg({\n",
    "        'NMI': ['mean', 'max', 'std'],\n",
    "        'ARI': ['mean', 'max', 'std'],\n",
    "        'Runtime_seconds': ['mean', 'min', 'max']\n",
    "    }).round(4)\n",
    "    print(feature_summary)\n",
    "    \n",
    "    # Ringkasan per algoritma\n",
    "    print(\"\\nRINGKASAN PER ALGORITMA:\")\n",
    "    print(\"-\" * 50)\n",
    "    algo_summary = results_df.groupby('Algorithm').agg({\n",
    "        'NMI': ['mean', 'max', 'std'],\n",
    "        'ARI': ['mean', 'max', 'std'],\n",
    "        'Runtime_seconds': ['mean', 'min', 'max']\n",
    "    }).round(4)\n",
    "    print(algo_summary)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Fungsi utama untuk menjalankan eksperimen\n",
    "def run_clustering_experiment(df, label_column='label', output_file='clustering_results.csv'):\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk menjalankan seluruh eksperimen\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame dengan fitur dan label\n",
    "    label_column: nama kolom yang berisi label sebenarnya\n",
    "    output_file: nama file output CSV\n",
    "    \"\"\"\n",
    "    print(\"Memulai Analisis Clustering untuk Author Attribution\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Informasi dataset\n",
    "    print(f\"Shape dataset: {df.shape}\")\n",
    "    print(f\"Kolom tersedia: {list(df.columns)}\")\n",
    "    \n",
    "    # Jalankan eksperimen\n",
    "    results_df = perform_clustering_experiment(df, label_column)\n",
    "    \n",
    "    # Analisis dan simpan hasil\n",
    "    final_results = analyze_and_save_results(results_df, output_file)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "\n",
    "results = run_clustering_experiment(df_olah, 'username', 'author_attribution_result_raw.csv')\n",
    "\n",
    "print(\"Script siap digunakan!\")\n",
    "print(\"Untuk menjalankan eksperimen, gunakan:\")\n",
    "print(\"results = run_clustering_experiment(df, 'nama_kolom_label', 'output_file_.csv')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
